{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 경로\n",
    "\n",
    "data_path = './porto-seguro-safe-driver-prediction/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv' , index_col = 'id')\n",
    "test = pd.read_csv(data_path + 'test.csv' , index_col = 'id')\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv' , index_col ='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "         ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n0                2              2          5              1              0   \n1                1              1          7              0              0   \n2                5              4          9              1              0   \n3                0              1          2              0              0   \n4                0              2          0              1              0   \n...            ...            ...        ...            ...            ...   \n1488023          0              1          6              0              0   \n1488024          5              3          5              1              0   \n1488025          0              1          5              0              0   \n1488026          6              1          5              1              0   \n1488027          7              1          4              1              0   \n\n         ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  \\\n0                    0              1              0              0   \n1                    0              0              1              0   \n2                    0              0              1              0   \n3                    1              0              0              0   \n4                    1              0              0              0   \n...                ...            ...            ...            ...   \n1488023              0              1              0              0   \n1488024              0              0              1              0   \n1488025              1              0              0              0   \n1488026              0              0              0              1   \n1488027              0              0              0              1   \n\n         ps_ind_10_bin  ...  ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  \\\n0                    0  ...           9           1           5           8   \n1                    0  ...           3           1           1           9   \n2                    0  ...           4           2           7           7   \n3                    0  ...           2           2           4           9   \n4                    0  ...           3           1           1           3   \n...                ...  ...         ...         ...         ...         ...   \n1488023              0  ...           4           2           3           4   \n1488024              0  ...           6           2           2          11   \n1488025              0  ...           5           2           2          11   \n1488026              0  ...           1           1           2           7   \n1488027              0  ...           5           2           2           7   \n\n         ps_calc_15_bin  ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  \\\n0                     0               1               1               0   \n1                     0               1               1               0   \n2                     0               1               1               0   \n3                     0               0               0               0   \n4                     0               0               0               1   \n...                 ...             ...             ...             ...   \n1488023               0               1               0               0   \n1488024               0               0               1               1   \n1488025               0               1               1               0   \n1488026               1               1               0               0   \n1488027               0               1               1               1   \n\n         ps_calc_19_bin  ps_calc_20_bin  \n0                     0               1  \n1                     1               0  \n2                     1               0  \n3                     0               0  \n4                     1               0  \n...                 ...             ...  \n1488023               1               0  \n1488024               0               0  \n1488025               0               0  \n1488026               0               0  \n1488027               0               0  \n\n[1488028 rows x 57 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ps_ind_01</th>\n      <th>ps_ind_02_cat</th>\n      <th>ps_ind_03</th>\n      <th>ps_ind_04_cat</th>\n      <th>ps_ind_05_cat</th>\n      <th>ps_ind_06_bin</th>\n      <th>ps_ind_07_bin</th>\n      <th>ps_ind_08_bin</th>\n      <th>ps_ind_09_bin</th>\n      <th>ps_ind_10_bin</th>\n      <th>...</th>\n      <th>ps_calc_11</th>\n      <th>ps_calc_12</th>\n      <th>ps_calc_13</th>\n      <th>ps_calc_14</th>\n      <th>ps_calc_15_bin</th>\n      <th>ps_calc_16_bin</th>\n      <th>ps_calc_17_bin</th>\n      <th>ps_calc_18_bin</th>\n      <th>ps_calc_19_bin</th>\n      <th>ps_calc_20_bin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>5</td>\n      <td>8</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>9</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>4</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>4</td>\n      <td>2</td>\n      <td>7</td>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1488023</th>\n      <td>0</td>\n      <td>1</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>4</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1488024</th>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>2</td>\n      <td>2</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1488025</th>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2</td>\n      <td>11</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1488026</th>\n      <td>6</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>7</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1488027</th>\n      <td>7</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2</td>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1488028 rows × 57 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.concat([train , test] , ignore_index= True)\n",
    "all_data = all_data.drop('target' , axis = 1) # 타깃값 제거\n",
    "\n",
    "all_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n       'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n       'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n       'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n       'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n       'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n       'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n       'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n       'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n       'ps_calc_20_bin'],\n      dtype='object')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = all_data.columns # 전체 피처\n",
    "all_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 명목형 피처 원-핫 인코딩"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<1488028x184 sparse matrix of type '<class 'numpy.float64'>'\n\twith 20832392 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 명목형 피처 추출\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "# 이름에 cat이 포함된 피처가 명목형 피처이다.\n",
    "onehot_encoder = OneHotEncoder() # 원-핫 인코더 객체 생성\n",
    "\n",
    "# 인코딩\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])\n",
    "\n",
    "encoded_cat_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 추가로 제거할 피처\n",
    "drop_features = ['ps_ind14' , 'ps_ind_10_bin' , 'ps_ind_11_bin' , 'ps_ind_12_bin' , 'ps_ind_13_bin' , 'ps_car_14' ]\n",
    "\n",
    "# 1> 명목형 피처 , 2> calc 분류의 피처 , 3> 추가 제거할 피처를 제외한 피처\n",
    "\n",
    "remaining_features = [feature for feature in all_features\n",
    "                      if ('cat' not in feature and\n",
    "                          'calc' not in feature and\n",
    "                          feature not in drop_features)]\n",
    "# cat(명목형) 피처 , calc(피처) , 추가 제거할 6개피처를 제외한 나머지 피처를 remaining_features 에 저장"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<1488028x202 sparse matrix of type '<class 'numpy.float64'>'\n\twith 37644877 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data[remaining_features]) , encoded_cat_matrix] , format= 'csr')\n",
    "\n",
    "all_data_sprs\n",
    "\n",
    "# CSR  형식으로 바꾸어 hstack()으로 행렬을 수평 방향으로 합친다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train :]\n",
    "\n",
    "y = train['target'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 지니계수"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 지니계수란?\n",
    "\n",
    "# 소득 불평등 정도를 나타내는 지표. 지니계수가 작을수록 소득 수준이 평등하고, 클수록 불평등함을 의미한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 정규화 지니계수 계산 함수\n",
    "\n",
    "# 정규화란 값의 범위를 0~1 사이로 조정한다는 의미, 정규화 지니계수는 값이 0에 가까울수록 성능이 나쁘고, 1에 가까울 수록 성능이 좋다는 의미가 된다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_gini(y_true , y_pred):\n",
    "    # 실제값과 예측값의 크기가 서로 같은지 확인(값이 다르면 오류 발생)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    n_samples = y_true.shape[0] # 데이터 개수\n",
    "    L_mid = np.linspace(1/ n_samples ,1 , n_samples) # 대각선 값\n",
    "\n",
    "    # 1) 예측값에 대한 지니계수\n",
    "\n",
    "    pred_order = y_true[y_pred.argsort()] # y_pred 크기순으로 y_true 값 정렬\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선\n",
    "\n",
    "    G_pred = np.sum(L_mid - L_pred) # 예측값에 대한 지니계수\n",
    "\n",
    "    # 2) 예측이 완벽할 때 지니계수\n",
    "\n",
    "    true_order = y_true[y_true.argsort()] # y_true 크기순으로 y_true 값 정렬\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order) # 로렌츠 곡선\n",
    "    G_true = np.sum(L_mid - L_true) # 예측이 완벽할 때 지니계수\n",
    "\n",
    "    # 정규화된 지니계수\n",
    "    return G_pred / G_true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# LightGBM 용 gini() 함수\n",
    "\n",
    "def gini(preds , dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "\n",
    "    return 'gini' , eval_gini(labels , preds ) , True\n",
    "\n",
    "# 'gini' : 평가지표이름 , eval_gini(labels,preds) : 평가점수 , True : 평가 점수가 높을수록 좋은지 여부"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 훈련 및 성능 검증"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# OOF(Out of Fold prediction) 예측 방식\n",
    "\n",
    "# K 폴드 교차 검증을 수행하면서 각 폴드마다 테스트 데이터로 예측하는 방식이다.\n",
    "\n",
    "# K 폴드 교차 검ㅈ응을 하면서 폴드마다 1> 훈련 데이터로 모델을 훈련하고, 2> 검증 데이터로 모델 성능을 측정하며 , 3> 테스트 데이터로 최종 타깃 확률도 예측한다. 훈련된 모델로 마지막에 한 번만 예측하는 것이 아니다. 각 폴드별 모델로 여러번 예측해 평균을 내는 방식이다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# OOF 방식으로 LightGBM 훈련\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기\n",
    "\n",
    "folds = StratifiedKFold(n_splits= 5 , shuffle= True , random_state= 1991)\n",
    "\n",
    "# 층화 K 폴드 교차 검증기는 타깃값이 불균형하므로 K폴드가 아닌 층화 K폴드를 수행하는 게 바람직하다. 층화 K폴드는 타깃값이 균등하게\n",
    "# 폴드를 나누는 방식이기 때문이다.\n",
    "\n",
    "\n",
    "# n_splits 파라미터로 전달한 수만큼 폴드를 나눈다. 여기서는 5개로 나누었다. shuffle = True 를 전달하면 폴드를 나눌때 데이터를 섞어준다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# LightGBM의 하이퍼파라미터를 설정한다. LightGBM은 하이퍼파라미터를 갖고 있지만, 여기서는 4가지만 설정한다.\n",
    "\n",
    "params = {'objective' : 'binary' , 'learning_rate' : 0.01 , 'force_row_wise' : True , 'random_state' : 0}\n",
    "\n",
    "# 이진분류 문제이므로 objective 파라미터는 binary로 설정했다. 학습률은 0.01로, 랜덤 스테이트 값은 9으로 설정했다.\n",
    "# force_row_wise : True 는 경고 문구를 없애려고 추가한 파라미터이다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "\n",
    "# ==> oof_val_preds 는 검증 데이터를 활용해 예측한 확률값을 저장하는 배열이다. K 폴드로 나누어도 훈련 데이터 전체가 결국엔 한 번씩 검증 데이터로 활용된다. 따라서 oof_val_preds 배열 크기는 훈련 데이터와 같아야 한다.\n",
    "# 훈련 데이터 개수는 X.shpae[0]으로 구한다.\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# oof_test_preds는 테스트 데이터를 활용해 예측한 확률값을 저장하는 배열이다. 최종 제출에 사용할 값이므로 크기는 테스트 데이터와 같아야한다. 테스트 데이터 개수는 X_test.shape[0]으로 구한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# !pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org lightgbm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1100\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153356\tvalid_0's gini: 0.261373\n",
      "[200]\tvalid_0's binary_logloss: 0.152424\tvalid_0's gini: 0.27589\n",
      "[300]\tvalid_0's binary_logloss: 0.152031\tvalid_0's gini: 0.281949\n",
      "[400]\tvalid_0's binary_logloss: 0.151802\tvalid_0's gini: 0.286194\n",
      "[500]\tvalid_0's binary_logloss: 0.151737\tvalid_0's gini: 0.286881\n",
      "[600]\tvalid_0's binary_logloss: 0.151686\tvalid_0's gini: 0.287701\n",
      "[700]\tvalid_0's binary_logloss: 0.151677\tvalid_0's gini: 0.287901\n",
      "Early stopping, best iteration is:\n",
      "[655]\tvalid_0's binary_logloss: 0.151674\tvalid_0's gini: 0.287982\n",
      "폴드 1 지니계수 : 0.2879824293420261\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153494\tvalid_0's gini: 0.249939\n",
      "[200]\tvalid_0's binary_logloss: 0.152711\tvalid_0's gini: 0.260705\n",
      "[300]\tvalid_0's binary_logloss: 0.152396\tvalid_0's gini: 0.267007\n",
      "[400]\tvalid_0's binary_logloss: 0.152249\tvalid_0's gini: 0.271043\n",
      "[500]\tvalid_0's binary_logloss: 0.152185\tvalid_0's gini: 0.272626\n",
      "[600]\tvalid_0's binary_logloss: 0.152165\tvalid_0's gini: 0.273257\n",
      "[700]\tvalid_0's binary_logloss: 0.152163\tvalid_0's gini: 0.27313\n",
      "Early stopping, best iteration is:\n",
      "[653]\tvalid_0's binary_logloss: 0.152158\tvalid_0's gini: 0.273406\n",
      "폴드 2 지니계수 : 0.2734064630206154\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153263\tvalid_0's gini: 0.261144\n",
      "[200]\tvalid_0's binary_logloss: 0.152341\tvalid_0's gini: 0.271587\n",
      "[300]\tvalid_0's binary_logloss: 0.151976\tvalid_0's gini: 0.276629\n",
      "[400]\tvalid_0's binary_logloss: 0.151818\tvalid_0's gini: 0.278742\n",
      "[500]\tvalid_0's binary_logloss: 0.151759\tvalid_0's gini: 0.279933\n",
      "[600]\tvalid_0's binary_logloss: 0.151759\tvalid_0's gini: 0.280056\n",
      "Early stopping, best iteration is:\n",
      "[541]\tvalid_0's binary_logloss: 0.151753\tvalid_0's gini: 0.280206\n",
      "폴드 3 지니계수 : 0.2802057321746898\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1100\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153399\tvalid_0's gini: 0.25056\n",
      "[200]\tvalid_0's binary_logloss: 0.152558\tvalid_0's gini: 0.262678\n",
      "[300]\tvalid_0's binary_logloss: 0.152249\tvalid_0's gini: 0.267308\n",
      "[400]\tvalid_0's binary_logloss: 0.152117\tvalid_0's gini: 0.269741\n",
      "[500]\tvalid_0's binary_logloss: 0.152071\tvalid_0's gini: 0.270738\n",
      "[600]\tvalid_0's binary_logloss: 0.152061\tvalid_0's gini: 0.27121\n",
      "[700]\tvalid_0's binary_logloss: 0.152069\tvalid_0's gini: 0.27145\n",
      "Early stopping, best iteration is:\n",
      "[608]\tvalid_0's binary_logloss: 0.152061\tvalid_0's gini: 0.271241\n",
      "폴드 4 지니계수 : 0.2712407835640063\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1103\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153481\tvalid_0's gini: 0.261934\n",
      "[200]\tvalid_0's binary_logloss: 0.152645\tvalid_0's gini: 0.273354\n",
      "[300]\tvalid_0's binary_logloss: 0.152268\tvalid_0's gini: 0.280299\n",
      "[400]\tvalid_0's binary_logloss: 0.152071\tvalid_0's gini: 0.284938\n",
      "[500]\tvalid_0's binary_logloss: 0.152005\tvalid_0's gini: 0.286724\n",
      "[600]\tvalid_0's binary_logloss: 0.151986\tvalid_0's gini: 0.28717\n",
      "[700]\tvalid_0's binary_logloss: 0.151979\tvalid_0's gini: 0.287158\n",
      "Early stopping, best iteration is:\n",
      "[658]\tvalid_0's binary_logloss: 0.151973\tvalid_0's gini: 0.287407\n",
      "폴드 5 지니계수 : 0.2874066737367479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# OOF 방식으로 모델 훈련 , 검증 , 예측\n",
    "\n",
    "for idx, (train_idx , valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40 , f'폴드 {idx +1} / 폴드 {folds.n_splits}' , '#'*40)\n",
    "\n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train , y_train = X[train_idx] , y[train_idx] # 훈련용 데이터\n",
    "    X_valid , y_valid = X[valid_idx] , y[valid_idx] # 검증용 데이터\n",
    "\n",
    "    # LightGBM 전용 데이터셋 생성\n",
    "    dtrain = lgb.Dataset(X_train , y_train) # LightGBM 전용 훈련 데이터 셋\n",
    "    dvalid = lgb.Dataset(X_valid , y_valid) # LightGBM 전용 검증 데이터 셋\n",
    "\n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params = params , # 훈련용 하이퍼파라미터\n",
    "                          train_set = dtrain, # 훈련 데이터 셋\n",
    "                          num_boost_round = 1000, # 부스팅 반복 횟수\n",
    "                          valid_sets=  dvalid ,  # 성능 평가용 검증 데이터 셋\n",
    "                          feval = gini, # 검증용 평가지표\n",
    "                          early_stopping_rounds = 100, # 조기종료 조건\n",
    "                          verbose_eval = 100 ) # 100번째마다 점수 출력\n",
    "\n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "\n",
    "    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n",
    "\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "\n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "\n",
    "    gini_score = eval_gini(y_valid , oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx +1} 지니계수 : {gini_score}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF 검증 데이터 지니계수 :  0.27992426994281666\n"
     ]
    }
   ],
   "source": [
    "print('OOF 검증 데이터 지니계수 : ' , eval_gini(y , oof_val_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 성능 개선"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n       'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n       'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n       'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n       'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n       'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n       'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n       'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n       'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n       'ps_calc_20_bin'],\n      dtype='object')"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.concat([train , test] , ignore_index = True)\n",
    "all_data = all_data.drop('target' , axis =1 ) # 타깃값 제거\n",
    "\n",
    "all_features = all_data.columns # 전체 피처\n",
    "\n",
    "all_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 명목형 피처\n",
    "\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "\n",
    "# 원-핫 인코딩 적용\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 파생 피처 추가"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# 탐색적 데이터 분석과정에서는 필요 없는 피처를 추리는 것 외에 특별한 피처 엔지니어링 건을 찾아내지 못했다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# 첫번째 , 한 데이터가 가진 결측값 개수를 파생 피처로 만들어본다. -1 이 결측값이므로 결측값 개수를 구하려면 -1 개수를 구하면 된다.\n",
    "\n",
    "# 데이터 하나당 결측값 개수를 파생 피처로 추가\n",
    "\n",
    "all_data['num_missing'] = (all_data == -1).sum(axis = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "['ps_ind_01',\n 'ps_ind_03',\n 'ps_ind_06_bin',\n 'ps_ind_07_bin',\n 'ps_ind_08_bin',\n 'ps_ind_09_bin',\n 'ps_ind_10_bin',\n 'ps_ind_11_bin',\n 'ps_ind_12_bin',\n 'ps_ind_13_bin',\n 'ps_ind_14',\n 'ps_ind_15',\n 'ps_ind_16_bin',\n 'ps_ind_17_bin',\n 'ps_ind_18_bin',\n 'ps_reg_01',\n 'ps_reg_02',\n 'ps_reg_03',\n 'ps_car_11',\n 'ps_car_12',\n 'ps_car_13',\n 'ps_car_14',\n 'ps_car_15',\n 'num_missing']"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명목형 피처 , calc 분류의 피처를 제외한 피처\n",
    "\n",
    "remaining_features = [feature for feature in all_features if ('cat' not in feature and 'calc' not in feature)]\n",
    "\n",
    "# num_missing을 remaining_features 에 추가\n",
    "\n",
    "remaining_features.append('num_missing')\n",
    "\n",
    "remaining_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "         ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n0                2              2          5              1              0   \n1                1              1          7              0              0   \n2                5              4          9              1              0   \n3                0              1          2              0              0   \n4                0              2          0              1              0   \n...            ...            ...        ...            ...            ...   \n1488023          0              1          6              0              0   \n1488024          5              3          5              1              0   \n1488025          0              1          5              0              0   \n1488026          6              1          5              1              0   \n1488027          7              1          4              1              0   \n\n         ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  \\\n0                    0              1              0              0   \n1                    0              0              1              0   \n2                    0              0              1              0   \n3                    1              0              0              0   \n4                    1              0              0              0   \n...                ...            ...            ...            ...   \n1488023              0              1              0              0   \n1488024              0              0              1              0   \n1488025              1              0              0              0   \n1488026              0              0              0              1   \n1488027              0              0              0              1   \n\n         ps_ind_10_bin  ...  ps_car_03_cat_count  ps_car_04_cat_count  \\\n0                    0  ...              1028142              1241334   \n1                    0  ...              1028142              1241334   \n2                    0  ...              1028142              1241334   \n3                    0  ...               183044              1241334   \n4                    0  ...              1028142              1241334   \n...                ...  ...                  ...                  ...   \n1488023              0  ...              1028142                51211   \n1488024              0  ...              1028142              1241334   \n1488025              0  ...              1028142              1241334   \n1488026              0  ...              1028142              1241334   \n1488027              0  ...               276842              1241334   \n\n         ps_car_05_cat_count  ps_car_06_cat_count  ps_car_07_cat_count  \\\n0                     431560                77845              1383070   \n1                     666910               329890              1383070   \n2                     666910               147714              1383070   \n3                     431560               329890              1383070   \n4                     666910               147714              1383070   \n...                      ...                  ...                  ...   \n1488023               389558               147714                76138   \n1488024               389558                83563              1383070   \n1488025               666910               329890              1383070   \n1488026               431560                83563              1383070   \n1488027               389558                77845              1383070   \n\n         ps_car_08_cat_count  ps_car_09_cat_count  ps_car_10_cat_count  \\\n0                     249663               486510              1475460   \n1                    1238365               883326              1475460   \n2                    1238365               883326              1475460   \n3                    1238365                36798              1475460   \n4                    1238365               883326              1475460   \n...                      ...                  ...                  ...   \n1488023              1238365               486510              1475460   \n1488024               249663               486510              1475460   \n1488025               249663                72947              1475460   \n1488026              1238365               486510              1475460   \n1488027              1238365               883326              1475460   \n\n         ps_car_11_cat_count  mix_ind_count  \n0                      18326              6  \n1                      12535             36  \n2                      19943             24  \n3                     212989           2784  \n4                      26161            258  \n...                      ...            ...  \n1488023                 3066            107  \n1488024                23506             26  \n1488025                 9725            258  \n1488026                31344             37  \n1488027                15198             67  \n\n[1488028 rows x 74 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ps_ind_01</th>\n      <th>ps_ind_02_cat</th>\n      <th>ps_ind_03</th>\n      <th>ps_ind_04_cat</th>\n      <th>ps_ind_05_cat</th>\n      <th>ps_ind_06_bin</th>\n      <th>ps_ind_07_bin</th>\n      <th>ps_ind_08_bin</th>\n      <th>ps_ind_09_bin</th>\n      <th>ps_ind_10_bin</th>\n      <th>...</th>\n      <th>ps_car_03_cat_count</th>\n      <th>ps_car_04_cat_count</th>\n      <th>ps_car_05_cat_count</th>\n      <th>ps_car_06_cat_count</th>\n      <th>ps_car_07_cat_count</th>\n      <th>ps_car_08_cat_count</th>\n      <th>ps_car_09_cat_count</th>\n      <th>ps_car_10_cat_count</th>\n      <th>ps_car_11_cat_count</th>\n      <th>mix_ind_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>431560</td>\n      <td>77845</td>\n      <td>1383070</td>\n      <td>249663</td>\n      <td>486510</td>\n      <td>1475460</td>\n      <td>18326</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>666910</td>\n      <td>329890</td>\n      <td>1383070</td>\n      <td>1238365</td>\n      <td>883326</td>\n      <td>1475460</td>\n      <td>12535</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>4</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>666910</td>\n      <td>147714</td>\n      <td>1383070</td>\n      <td>1238365</td>\n      <td>883326</td>\n      <td>1475460</td>\n      <td>19943</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>183044</td>\n      <td>1241334</td>\n      <td>431560</td>\n      <td>329890</td>\n      <td>1383070</td>\n      <td>1238365</td>\n      <td>36798</td>\n      <td>1475460</td>\n      <td>212989</td>\n      <td>2784</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>666910</td>\n      <td>147714</td>\n      <td>1383070</td>\n      <td>1238365</td>\n      <td>883326</td>\n      <td>1475460</td>\n      <td>26161</td>\n      <td>258</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1488023</th>\n      <td>0</td>\n      <td>1</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>51211</td>\n      <td>389558</td>\n      <td>147714</td>\n      <td>76138</td>\n      <td>1238365</td>\n      <td>486510</td>\n      <td>1475460</td>\n      <td>3066</td>\n      <td>107</td>\n    </tr>\n    <tr>\n      <th>1488024</th>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>389558</td>\n      <td>83563</td>\n      <td>1383070</td>\n      <td>249663</td>\n      <td>486510</td>\n      <td>1475460</td>\n      <td>23506</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>1488025</th>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>666910</td>\n      <td>329890</td>\n      <td>1383070</td>\n      <td>249663</td>\n      <td>72947</td>\n      <td>1475460</td>\n      <td>9725</td>\n      <td>258</td>\n    </tr>\n    <tr>\n      <th>1488026</th>\n      <td>6</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1028142</td>\n      <td>1241334</td>\n      <td>431560</td>\n      <td>83563</td>\n      <td>1383070</td>\n      <td>1238365</td>\n      <td>486510</td>\n      <td>1475460</td>\n      <td>31344</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>1488027</th>\n      <td>7</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>276842</td>\n      <td>1241334</td>\n      <td>389558</td>\n      <td>77845</td>\n      <td>1383070</td>\n      <td>1238365</td>\n      <td>883326</td>\n      <td>1475460</td>\n      <td>15198</td>\n      <td>67</td>\n    </tr>\n  </tbody>\n</table>\n<p>1488028 rows × 74 columns</p>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# 두번째, ind 분류의 피처들을 살펴본다. 모든 ind 피처 값을 연결해서 새로운 피처를 만들려고한다.\n",
    "\n",
    "# 예를들어 , ps_ind_01 , ps_ind_02_car , ps_ind_03의 값이 각각 2, 3, 5 라면 모든 값을 연결해 2_2_5_로 만든다.\n",
    "\n",
    "# ind 피처가 총 18개이므로 18개 값이 연결된다.\n",
    "\n",
    "ind_features = [feature for feature in all_features if 'ind' in feature]\n",
    "\n",
    "is_first_feature = True\n",
    "\n",
    "for ind_feature in ind_features:\n",
    "    if is_first_feature:\n",
    "        all_data['mix_ind'] = all_data[ind_feature].astype(str) + '-'\n",
    "        is_first_feature = False\n",
    "    else:\n",
    "        all_data['mix_ind'] += all_data[ind_feature].astype(str) + '-'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "0          2-2-5-1-0-0-1-0-0-0-0-0-0-0-11-0-1-0-\n1           1-1-7-0-0-0-0-1-0-0-0-0-0-0-3-0-0-1-\n2          5-4-9-1-0-0-0-1-0-0-0-0-0-0-12-1-0-0-\n3           0-1-2-0-0-1-0-0-0-0-0-0-0-0-8-1-0-0-\n4           0-2-0-1-0-1-0-0-0-0-0-0-0-0-9-1-0-0-\n                           ...                  \n1488023     0-1-6-0-0-0-1-0-0-0-0-0-0-0-2-0-0-1-\n1488024    5-3-5-1-0-0-0-1-0-0-0-0-0-0-11-1-0-0-\n1488025     0-1-5-0-0-1-0-0-0-0-0-0-0-0-5-0-0-1-\n1488026    6-1-5-1-0-0-0-0-1-0-0-0-0-0-13-1-0-0-\n1488027    7-1-4-1-0-0-0-0-1-0-0-0-0-0-12-1-0-0-\nName: mix_ind, Length: 1488028, dtype: object"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['mix_ind']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": " 1    1079327\n 2     309747\n 3      70172\n 4      28259\n-1        523\nName: ps_ind_02_cat, dtype: int64"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 세번째 , 명목형 피처의 고윳값별 개수를 새로운 피처로 추가한다. 고윳값별 개수는 value_counts()로 구한다.\n",
    "\n",
    "# ps_ind_02_cat 피처의 고윳값별 개수 코드\n",
    "\n",
    "all_data['ps_ind_02_cat'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "{1: 1079327, 2: 309747, 3: 70172, 4: 28259, -1: 523}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명목형 피처의 고윳값별 개수를 파생 피처로 생성\n",
    "all_data['ps_ind_02_cat'].value_counts().to_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "cat_count_features = []\n",
    "\n",
    "for feature in cat_features+['mix_ind']:\n",
    "    val_counts_dict = all_data[feature].value_counts().to_dict()\n",
    "    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x: val_counts_dict[x])\n",
    "\n",
    "    cat_count_features.append(f'{feature}_count')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "['ps_ind_02_cat_count',\n 'ps_ind_04_cat_count',\n 'ps_ind_05_cat_count',\n 'ps_car_01_cat_count',\n 'ps_car_02_cat_count',\n 'ps_car_03_cat_count',\n 'ps_car_04_cat_count',\n 'ps_car_05_cat_count',\n 'ps_car_06_cat_count',\n 'ps_car_07_cat_count',\n 'ps_car_08_cat_count',\n 'ps_car_09_cat_count',\n 'ps_car_10_cat_count',\n 'ps_car_11_cat_count',\n 'mix_ind_count']"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_count_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 필요 없는 피처들\n",
    "drop_features = ['ps_ind_14' , 'ps_ind_10_bin' , 'ps_ind_11_bin' , 'ps_ind_12_bin' , 'ps_ind_13_bin' , 'ps_car_14']\n",
    "\n",
    "# remaining_features , cat_count_features 에서 drop_features를 제거한 데이터\n",
    "\n",
    "all_data_remaining = all_data[remaining_features + cat_count_features].drop(drop_features , axis = 1)\n",
    "\n",
    "# 데이터 합치기\n",
    "\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining) , encoded_cat_matrix] ,format = 'csr')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터 나누기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train :]\n",
    "\n",
    "y = train['target'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 하이퍼파라미터 최적화"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리(베이지안 최적화 수행용)\n",
    "\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X,y, test_size=0.2 , random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "\n",
    "bayes_dtrain = lgb.Dataset(X_train , y_train)\n",
    "bayes_dvalid = lgb.Dataset(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'num_leaves' : (30 , 40) , # 개별 트리가 가질 수 있는 최대 말단 노드 개수 , 트리 복잡도 결정, 값이 클수록 좋다.\n",
    "                'lambda_l1' : (0.7 , 0.9), # L1 규제 조정값 , 값이 클수록 과대적합 방지 효과\n",
    "                'lambda_l2' : (0.9 , 1), # L2 규제 조정값 , 값이 클수록 과대적합 방지 효과\n",
    "                'feature_fraction' : (0.6 , 0.7), # 개별 트리를 훈련할 때 사용할 피처 샘플링 비율\n",
    "                'bagging_fraction' : (0.6 , 0.9), # 개별 트리를 훈련할 때 사용할 데이터 샘플링 비율\n",
    "                'min_child_samples' : (6 , 10) , # 말단 노드가 되기 위해 필요한 최소 데이터 개수 , 값이 클수록 과대적합 방지\n",
    "                'min_child_weight' : (10 , 40)} # 과대적합 방지 위한 값\n",
    "\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "\n",
    "fixed_params = {'objective' : 'binary' , # 훈련 목적 , 회귀에서는 'regression' , 이진분류에서는 'binary' , 다중분류에서는 'multiclass' 사용\n",
    "                'learning_rate' : 0.005, # 학습률( 부스팅 이터레이션을 반복하면서 모델을 업데이트하는 데 사용 되는 비율)\n",
    "                'bagging_freq' : 1, # 배깅 수행 빈도, 몇번의 이터레이션마다 배깅 수행할 지 결정\n",
    "                'force_row_wise' : True, # 메모리 용량이 충분하지 않을 때 메모리 효율을 높이는 파라미터\n",
    "                'random_state' : 1991} # 랜덤 시드값 (코드를 반복 실행해도 같은 결과가 나오게 지정하는 값)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 베이지안 최적화용 평가지표 계산 함수 작성"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def eval_function(num_leaves , lambda_l1 , lambda_l2 , feature_fraction , bagging_fraction , min_child_samples , min_child_weight) :\n",
    "\n",
    "# 최적화하려는 평가지표(지니계수) 계산 함수\n",
    "\n",
    "# 베이지안 최적화를 수행할 하이퍼파라미터\n",
    "\n",
    "    params = {'num_leaves' : int(round(num_leaves)) , # 개발 트리가 가질 수 있는 최대 말단 노드 개수, 트리 복잡도 결정 , 값이 클수록 좋다.\n",
    "              'lambda_l1' : lambda_l1, # L1 규제 조정값 , 값이 클 수록 과대적합 방지 효과\n",
    "              'lambda_l2' : lambda_l2 , # L2 규제 조정값 , 값이 클 수록 과대적합 방지 효과\n",
    "              'feature_fraction' : feature_fraction ,  # 개별 트리를 훈련할 때 사용할 피처 샘플링 비율\n",
    "              'bagging_fraction' : bagging_fraction, # 개별 트리를 훈련할 때 사용할 배깅 데이터 샘플링 비율\n",
    "              'min_child_samples' : int(round(min_child_samples)) , # 말단 노드가 되기 위해 필요한 최소 데이터 개수, 값이 클수록 과대적합 방지\n",
    "              'min_child_weight' : min_child_weight, # 과대적합 방지 위한 값\n",
    "              'feature_pre_filter' : False} #\n",
    "\n",
    "    #하이퍼파라미터도 추가\n",
    "    params.update(fixed_params)\n",
    "\n",
    "    print('하이퍼파라미터 : ' , params)\n",
    "\n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params = params , # 훈련용 하이퍼파라미터\n",
    "                          train_set = bayes_dtrain, # 훈련 데이터셋\n",
    "                          num_boost_round= 2500, #부스팅 반복횟수\n",
    "                          valid_sets= bayes_dvalid, # 성능 평가용 검증 데이터 셋\n",
    "                          feval = gini, # 검증용 평가지표\n",
    "                          early_stopping_rounds= 300, # 조기종료 조건\n",
    "                          verbose_eval= False) # 계속 점수 출력\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = lgb_model.predict(X_valid)\n",
    "\n",
    "    # 지니계수 계산\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "\n",
    "    return gini_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 최적화 수행"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colorama==0.4.4 in c:\\users\\andyp\\anaconda3\\lib\\site-packages (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org colorama==0.4.4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization==1.4.0 in c:\\users\\andyp\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\andyp\\anaconda3\\lib\\site-packages (from bayesian-optimization==1.4.0) (0.24.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\andyp\\anaconda3\\lib\\site-packages (from bayesian-optimization==1.4.0) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\andyp\\anaconda3\\lib\\site-packages (from bayesian-optimization==1.4.0) (1.20.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\andyp\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization==1.4.0) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\andyp\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization==1.4.0) (2.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org bayesian-optimization==1.4.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "optimizer = BayesianOptimization(f = eval_function, # 평가지표 계산 함수\n",
    "                                pbounds = param_bounds, # 하이퍼파라미터 범위\n",
    "                                random_state = 0 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터 :  {'num_leaves': 34, 'lambda_l1': 0.8205526752143287, 'lambda_l2': 0.9544883182996897, 'feature_fraction': 0.6715189366372419, 'bagging_fraction': 0.7646440511781974, 'min_child_samples': 8, 'min_child_weight': 29.376823391999682, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2855811556220905\n",
      "\n",
      "| \u001B[0m1        \u001B[0m | \u001B[0m0.2856   \u001B[0m | \u001B[0m0.7646   \u001B[0m | \u001B[0m0.6715   \u001B[0m | \u001B[0m0.8206   \u001B[0m | \u001B[0m0.9545   \u001B[0m | \u001B[0m7.695    \u001B[0m | \u001B[0m29.38    \u001B[0m | \u001B[0m34.38    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 39, 'lambda_l1': 0.7766883037651555, 'lambda_l2': 0.9791725038082665, 'feature_fraction': 0.6963662760501029, 'bagging_fraction': 0.867531900234624, 'min_child_samples': 8, 'min_child_weight': 27.04133683281797, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2837380537005777\n",
      "\n",
      "| \u001B[0m2        \u001B[0m | \u001B[0m0.2837   \u001B[0m | \u001B[0m0.8675   \u001B[0m | \u001B[0m0.6964   \u001B[0m | \u001B[0m0.7767   \u001B[0m | \u001B[0m0.9792   \u001B[0m | \u001B[0m8.116    \u001B[0m | \u001B[0m27.04    \u001B[0m | \u001B[0m39.26    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 40, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'feature_fraction': 0.608712929970154, 'bagging_fraction': 0.6213108174593661, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2857848354322048\n",
      "\n",
      "| \u001B[95m3        \u001B[0m | \u001B[95m0.2858   \u001B[0m | \u001B[95m0.6213   \u001B[0m | \u001B[95m0.6087   \u001B[0m | \u001B[95m0.704    \u001B[0m | \u001B[95m0.9833   \u001B[0m | \u001B[95m9.113    \u001B[0m | \u001B[95m36.1     \u001B[0m | \u001B[95m39.79    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 30, 'lambda_l1': 0.8444997594874222, 'lambda_l2': 0.9234023852202012, 'feature_fraction': 0.6593983245038058, 'bagging_fraction': 0.8977977822397395, 'min_child_samples': 9, 'min_child_weight': 10.549362495448534, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2828993761731121\n",
      "\n",
      "| \u001B[0m4        \u001B[0m | \u001B[0m0.2829   \u001B[0m | \u001B[0m0.8978   \u001B[0m | \u001B[0m0.6594   \u001B[0m | \u001B[0m0.8445   \u001B[0m | \u001B[0m0.9234   \u001B[0m | \u001B[0m8.619    \u001B[0m | \u001B[0m10.55    \u001B[0m | \u001B[0m30.09    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 37, 'lambda_l1': 0.7738449330497988, 'lambda_l2': 0.9032695189818599, 'feature_fraction': 0.6606341064409726, 'bagging_fraction': 0.7666713964943057, 'min_child_samples': 9, 'min_child_weight': 29.306172421380474, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28513273331754563\n",
      "\n",
      "| \u001B[0m5        \u001B[0m | \u001B[0m0.2851   \u001B[0m | \u001B[0m0.7667   \u001B[0m | \u001B[0m0.6606   \u001B[0m | \u001B[0m0.7738   \u001B[0m | \u001B[0m0.9033   \u001B[0m | \u001B[0m8.769    \u001B[0m | \u001B[0m29.31    \u001B[0m | \u001B[0m36.6     \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 33, 'lambda_l1': 0.8178523882153511, 'lambda_l2': 0.9, 'feature_fraction': 0.6, 'bagging_fraction': 0.6, 'min_child_samples': 10, 'min_child_weight': 35.79651643178398, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2853891658385996\n",
      "\n",
      "| \u001B[0m6        \u001B[0m | \u001B[0m0.2854   \u001B[0m | \u001B[0m0.6      \u001B[0m | \u001B[0m0.6      \u001B[0m | \u001B[0m0.8179   \u001B[0m | \u001B[0m0.9      \u001B[0m | \u001B[0m9.967    \u001B[0m | \u001B[0m35.8     \u001B[0m | \u001B[0m32.59    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 37, 'lambda_l1': 0.8433793375135147, 'lambda_l2': 0.9479651949974717, 'feature_fraction': 0.6859622896374784, 'bagging_fraction': 0.8362539818721497, 'min_child_samples': 6, 'min_child_weight': 39.77484183530247, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2854766974907317\n",
      "\n",
      "| \u001B[0m7        \u001B[0m | \u001B[0m0.2855   \u001B[0m | \u001B[0m0.8363   \u001B[0m | \u001B[0m0.686    \u001B[0m | \u001B[0m0.8434   \u001B[0m | \u001B[0m0.948    \u001B[0m | \u001B[0m6.002    \u001B[0m | \u001B[0m39.77    \u001B[0m | \u001B[0m36.8     \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 30, 'lambda_l1': 0.7759269600824816, 'lambda_l2': 0.9, 'feature_fraction': 0.7, 'bagging_fraction': 0.6, 'min_child_samples': 10, 'min_child_weight': 28.21283616384366, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2847964155782387\n",
      "\n",
      "| \u001B[0m8        \u001B[0m | \u001B[0m0.2848   \u001B[0m | \u001B[0m0.6      \u001B[0m | \u001B[0m0.7      \u001B[0m | \u001B[0m0.7759   \u001B[0m | \u001B[0m0.9      \u001B[0m | \u001B[0m10.0     \u001B[0m | \u001B[0m28.21    \u001B[0m | \u001B[0m30.0     \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 36, 'lambda_l1': 0.7, 'lambda_l2': 1.0, 'feature_fraction': 0.7, 'bagging_fraction': 0.9, 'min_child_samples': 6, 'min_child_weight': 33.98699093132045, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.283740750776938\n",
      "\n",
      "| \u001B[0m9        \u001B[0m | \u001B[0m0.2837   \u001B[0m | \u001B[0m0.9      \u001B[0m | \u001B[0m0.7      \u001B[0m | \u001B[0m0.7      \u001B[0m | \u001B[0m1.0      \u001B[0m | \u001B[0m6.0      \u001B[0m | \u001B[0m33.99    \u001B[0m | \u001B[0m35.84    \u001B[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 베이지안 최적화 수행\n",
    "\n",
    "optimizer.maximize(init_points=  3 , n_iter = 6) # init_points 는 무작위로 하이퍼파라미터를 탐색하는 횟수, n_iter는 베이지안 최적화 반복 횟수"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 결과확인"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "{'bagging_fraction': 0.6213108174593661,\n 'feature_fraction': 0.608712929970154,\n 'lambda_l1': 0.7040436794880651,\n 'lambda_l2': 0.9832619845547939,\n 'min_child_samples': 9.112627003799401,\n 'min_child_weight': 36.10036444740457,\n 'num_leaves': 39.78618342232764}"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가함수 점수가 최대일 대 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# 정수형 하이퍼파라미터 변환\n",
    "\n",
    "max_params['num_leaves'] = int(round(max_params['num_leaves']))\n",
    "max_params['min_child_samples'] = int(round(max_params['min_child_samples']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# 값이 고정된 하이퍼파라미터 추가\n",
    "\n",
    "max_params.update(fixed_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "{'bagging_fraction': 0.6213108174593661,\n 'feature_fraction': 0.608712929970154,\n 'lambda_l1': 0.7040436794880651,\n 'lambda_l2': 0.9832619845547939,\n 'min_child_samples': 9,\n 'min_child_weight': 36.10036444740457,\n 'num_leaves': 40,\n 'objective': 'binary',\n 'learning_rate': 0.005,\n 'bagging_freq': 1,\n 'force_row_wise': True,\n 'random_state': 1991}"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최종 하이퍼파라미터 출력\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 훈련 및 성능 검증"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153929\tvalid_0's gini: 0.297826\n",
      "[200]\tvalid_0's binary_logloss: 0.152575\tvalid_0's gini: 0.307987\n",
      "[300]\tvalid_0's binary_logloss: 0.151732\tvalid_0's gini: 0.315646\n",
      "[400]\tvalid_0's binary_logloss: 0.15115\tvalid_0's gini: 0.322832\n",
      "[500]\tvalid_0's binary_logloss: 0.150688\tvalid_0's gini: 0.330498\n",
      "[600]\tvalid_0's binary_logloss: 0.150302\tvalid_0's gini: 0.337661\n",
      "[700]\tvalid_0's binary_logloss: 0.149967\tvalid_0's gini: 0.344703\n",
      "[800]\tvalid_0's binary_logloss: 0.149665\tvalid_0's gini: 0.351174\n",
      "[900]\tvalid_0's binary_logloss: 0.149386\tvalid_0's gini: 0.357542\n",
      "[1000]\tvalid_0's binary_logloss: 0.14914\tvalid_0's gini: 0.363011\n",
      "[1100]\tvalid_0's binary_logloss: 0.1489\tvalid_0's gini: 0.368648\n",
      "[1200]\tvalid_0's binary_logloss: 0.148671\tvalid_0's gini: 0.373879\n",
      "[1300]\tvalid_0's binary_logloss: 0.148457\tvalid_0's gini: 0.378876\n",
      "[1400]\tvalid_0's binary_logloss: 0.148254\tvalid_0's gini: 0.383529\n",
      "[1500]\tvalid_0's binary_logloss: 0.148053\tvalid_0's gini: 0.388412\n",
      "[1600]\tvalid_0's binary_logloss: 0.147855\tvalid_0's gini: 0.393132\n",
      "[1700]\tvalid_0's binary_logloss: 0.147671\tvalid_0's gini: 0.397335\n",
      "[1800]\tvalid_0's binary_logloss: 0.147479\tvalid_0's gini: 0.401812\n",
      "[1900]\tvalid_0's binary_logloss: 0.147298\tvalid_0's gini: 0.406149\n",
      "[2000]\tvalid_0's binary_logloss: 0.147113\tvalid_0's gini: 0.410341\n",
      "[2100]\tvalid_0's binary_logloss: 0.146935\tvalid_0's gini: 0.414366\n",
      "[2200]\tvalid_0's binary_logloss: 0.146751\tvalid_0's gini: 0.418657\n",
      "[2300]\tvalid_0's binary_logloss: 0.146582\tvalid_0's gini: 0.422601\n",
      "[2400]\tvalid_0's binary_logloss: 0.146412\tvalid_0's gini: 0.426673\n",
      "[2500]\tvalid_0's binary_logloss: 0.146249\tvalid_0's gini: 0.430452\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's binary_logloss: 0.146249\tvalid_0's gini: 0.430452\n",
      "폴드 1  지니계수 : 0.43045153631463173\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154044\tvalid_0's gini: 0.285099\n",
      "[200]\tvalid_0's binary_logloss: 0.152769\tvalid_0's gini: 0.294886\n",
      "[300]\tvalid_0's binary_logloss: 0.151986\tvalid_0's gini: 0.302327\n",
      "[400]\tvalid_0's binary_logloss: 0.151437\tvalid_0's gini: 0.310055\n",
      "[500]\tvalid_0's binary_logloss: 0.151012\tvalid_0's gini: 0.317354\n",
      "[600]\tvalid_0's binary_logloss: 0.150656\tvalid_0's gini: 0.324008\n",
      "[700]\tvalid_0's binary_logloss: 0.150342\tvalid_0's gini: 0.330635\n",
      "[800]\tvalid_0's binary_logloss: 0.150055\tvalid_0's gini: 0.336887\n",
      "[900]\tvalid_0's binary_logloss: 0.149793\tvalid_0's gini: 0.34274\n",
      "[1000]\tvalid_0's binary_logloss: 0.149536\tvalid_0's gini: 0.348577\n",
      "[1100]\tvalid_0's binary_logloss: 0.149294\tvalid_0's gini: 0.354284\n",
      "[1200]\tvalid_0's binary_logloss: 0.149065\tvalid_0's gini: 0.359531\n",
      "[1300]\tvalid_0's binary_logloss: 0.148847\tvalid_0's gini: 0.36461\n",
      "[1400]\tvalid_0's binary_logloss: 0.148643\tvalid_0's gini: 0.369238\n",
      "[1500]\tvalid_0's binary_logloss: 0.148446\tvalid_0's gini: 0.373856\n",
      "[1600]\tvalid_0's binary_logloss: 0.148244\tvalid_0's gini: 0.378513\n",
      "[1700]\tvalid_0's binary_logloss: 0.148049\tvalid_0's gini: 0.383028\n",
      "[1800]\tvalid_0's binary_logloss: 0.14786\tvalid_0's gini: 0.387413\n",
      "[1900]\tvalid_0's binary_logloss: 0.14767\tvalid_0's gini: 0.391896\n",
      "[2000]\tvalid_0's binary_logloss: 0.147487\tvalid_0's gini: 0.396185\n",
      "[2100]\tvalid_0's binary_logloss: 0.147311\tvalid_0's gini: 0.400129\n",
      "[2200]\tvalid_0's binary_logloss: 0.147136\tvalid_0's gini: 0.40407\n",
      "[2300]\tvalid_0's binary_logloss: 0.146961\tvalid_0's gini: 0.40802\n",
      "[2400]\tvalid_0's binary_logloss: 0.14679\tvalid_0's gini: 0.412142\n",
      "[2500]\tvalid_0's binary_logloss: 0.146622\tvalid_0's gini: 0.416\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's binary_logloss: 0.146622\tvalid_0's gini: 0.416\n",
      "폴드 2  지니계수 : 0.41599986558989005\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153916\tvalid_0's gini: 0.291575\n",
      "[200]\tvalid_0's binary_logloss: 0.152573\tvalid_0's gini: 0.300826\n",
      "[300]\tvalid_0's binary_logloss: 0.151731\tvalid_0's gini: 0.308526\n",
      "[400]\tvalid_0's binary_logloss: 0.151146\tvalid_0's gini: 0.316146\n",
      "[500]\tvalid_0's binary_logloss: 0.150697\tvalid_0's gini: 0.323212\n",
      "[600]\tvalid_0's binary_logloss: 0.150325\tvalid_0's gini: 0.329874\n",
      "[700]\tvalid_0's binary_logloss: 0.150007\tvalid_0's gini: 0.336197\n",
      "[800]\tvalid_0's binary_logloss: 0.149721\tvalid_0's gini: 0.342142\n",
      "[900]\tvalid_0's binary_logloss: 0.14946\tvalid_0's gini: 0.347916\n",
      "[1000]\tvalid_0's binary_logloss: 0.149224\tvalid_0's gini: 0.353072\n",
      "[1100]\tvalid_0's binary_logloss: 0.149002\tvalid_0's gini: 0.358182\n",
      "[1200]\tvalid_0's binary_logloss: 0.14878\tvalid_0's gini: 0.363267\n",
      "[1300]\tvalid_0's binary_logloss: 0.148571\tvalid_0's gini: 0.368104\n",
      "[1400]\tvalid_0's binary_logloss: 0.148376\tvalid_0's gini: 0.372453\n",
      "[1500]\tvalid_0's binary_logloss: 0.148179\tvalid_0's gini: 0.37712\n",
      "[1600]\tvalid_0's binary_logloss: 0.147985\tvalid_0's gini: 0.38163\n",
      "[1700]\tvalid_0's binary_logloss: 0.147796\tvalid_0's gini: 0.386097\n",
      "[1800]\tvalid_0's binary_logloss: 0.147605\tvalid_0's gini: 0.3905\n",
      "[1900]\tvalid_0's binary_logloss: 0.14743\tvalid_0's gini: 0.394776\n",
      "[2000]\tvalid_0's binary_logloss: 0.147257\tvalid_0's gini: 0.398703\n",
      "[2100]\tvalid_0's binary_logloss: 0.147083\tvalid_0's gini: 0.402504\n",
      "[2200]\tvalid_0's binary_logloss: 0.14691\tvalid_0's gini: 0.406516\n",
      "[2300]\tvalid_0's binary_logloss: 0.146729\tvalid_0's gini: 0.410652\n",
      "[2400]\tvalid_0's binary_logloss: 0.14656\tvalid_0's gini: 0.414681\n",
      "[2500]\tvalid_0's binary_logloss: 0.14639\tvalid_0's gini: 0.418588\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's binary_logloss: 0.14639\tvalid_0's gini: 0.418588\n",
      "폴드 3  지니계수 : 0.4185875528897569\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154037\tvalid_0's gini: 0.283031\n",
      "[200]\tvalid_0's binary_logloss: 0.152752\tvalid_0's gini: 0.292404\n",
      "[300]\tvalid_0's binary_logloss: 0.151959\tvalid_0's gini: 0.299947\n",
      "[400]\tvalid_0's binary_logloss: 0.151401\tvalid_0's gini: 0.307543\n",
      "[500]\tvalid_0's binary_logloss: 0.150978\tvalid_0's gini: 0.31462\n",
      "[600]\tvalid_0's binary_logloss: 0.150627\tvalid_0's gini: 0.321184\n",
      "[700]\tvalid_0's binary_logloss: 0.150317\tvalid_0's gini: 0.327634\n",
      "[800]\tvalid_0's binary_logloss: 0.150029\tvalid_0's gini: 0.334011\n",
      "[900]\tvalid_0's binary_logloss: 0.149769\tvalid_0's gini: 0.339993\n",
      "[1000]\tvalid_0's binary_logloss: 0.149521\tvalid_0's gini: 0.345545\n",
      "[1100]\tvalid_0's binary_logloss: 0.149293\tvalid_0's gini: 0.351003\n",
      "[1200]\tvalid_0's binary_logloss: 0.149073\tvalid_0's gini: 0.356248\n",
      "[1300]\tvalid_0's binary_logloss: 0.148871\tvalid_0's gini: 0.361182\n",
      "[1400]\tvalid_0's binary_logloss: 0.148677\tvalid_0's gini: 0.365689\n",
      "[1500]\tvalid_0's binary_logloss: 0.148476\tvalid_0's gini: 0.370684\n",
      "[1600]\tvalid_0's binary_logloss: 0.148287\tvalid_0's gini: 0.375168\n",
      "[1700]\tvalid_0's binary_logloss: 0.148096\tvalid_0's gini: 0.379801\n",
      "[1800]\tvalid_0's binary_logloss: 0.147914\tvalid_0's gini: 0.384194\n",
      "[1900]\tvalid_0's binary_logloss: 0.14773\tvalid_0's gini: 0.388672\n",
      "[2000]\tvalid_0's binary_logloss: 0.147553\tvalid_0's gini: 0.392803\n",
      "[2100]\tvalid_0's binary_logloss: 0.147369\tvalid_0's gini: 0.397084\n",
      "[2200]\tvalid_0's binary_logloss: 0.147192\tvalid_0's gini: 0.40121\n",
      "[2300]\tvalid_0's binary_logloss: 0.147016\tvalid_0's gini: 0.405269\n",
      "[2400]\tvalid_0's binary_logloss: 0.146852\tvalid_0's gini: 0.409275\n",
      "[2500]\tvalid_0's binary_logloss: 0.146681\tvalid_0's gini: 0.413347\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's binary_logloss: 0.146681\tvalid_0's gini: 0.413347\n",
      "폴드 4  지니계수 : 0.4133467618166041\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.15439\tvalid_0's gini: 0.26681\n",
      "[200]\tvalid_0's binary_logloss: 0.15338\tvalid_0's gini: 0.272186\n",
      "[300]\tvalid_0's binary_logloss: 0.152821\tvalid_0's gini: 0.275897\n",
      "[400]\tvalid_0's binary_logloss: 0.1525\tvalid_0's gini: 0.278734\n",
      "[500]\tvalid_0's binary_logloss: 0.152277\tvalid_0's gini: 0.282151\n",
      "[600]\tvalid_0's binary_logloss: 0.15212\tvalid_0's gini: 0.285039\n",
      "[700]\tvalid_0's binary_logloss: 0.152009\tvalid_0's gini: 0.287435\n",
      "[800]\tvalid_0's binary_logloss: 0.15192\tvalid_0's gini: 0.289549\n",
      "[900]\tvalid_0's binary_logloss: 0.151862\tvalid_0's gini: 0.290886\n",
      "[1000]\tvalid_0's binary_logloss: 0.151819\tvalid_0's gini: 0.291935\n",
      "[1100]\tvalid_0's binary_logloss: 0.151782\tvalid_0's gini: 0.292972\n",
      "[1200]\tvalid_0's binary_logloss: 0.151752\tvalid_0's gini: 0.293784\n",
      "[1300]\tvalid_0's binary_logloss: 0.151732\tvalid_0's gini: 0.294315\n",
      "[1400]\tvalid_0's binary_logloss: 0.151724\tvalid_0's gini: 0.294475\n",
      "[1500]\tvalid_0's binary_logloss: 0.151713\tvalid_0's gini: 0.294786\n",
      "[1600]\tvalid_0's binary_logloss: 0.1517\tvalid_0's gini: 0.295146\n",
      "[1700]\tvalid_0's binary_logloss: 0.151694\tvalid_0's gini: 0.295268\n",
      "[1800]\tvalid_0's binary_logloss: 0.151695\tvalid_0's gini: 0.295212\n",
      "[1900]\tvalid_0's binary_logloss: 0.151689\tvalid_0's gini: 0.295454\n",
      "[2000]\tvalid_0's binary_logloss: 0.151693\tvalid_0's gini: 0.2954\n",
      "[2100]\tvalid_0's binary_logloss: 0.151694\tvalid_0's gini: 0.295427\n",
      "[2200]\tvalid_0's binary_logloss: 0.151692\tvalid_0's gini: 0.295538\n",
      "[2300]\tvalid_0's binary_logloss: 0.151699\tvalid_0's gini: 0.295411\n",
      "Early stopping, best iteration is:\n",
      "[2045]\tvalid_0's binary_logloss: 0.151689\tvalid_0's gini: 0.295553\n",
      "폴드 5  지니계수 : 0.29555250456072807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기 생성\n",
    "folds = StratifiedKFold(n_splits=5 , shuffle = True , random_state= 1991)\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# OOF 방식으로 모델 훈련 ,검증 , 예측\n",
    "\n",
    "for idx, (train , valid_idx) in enumerate(folds.split(X,y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40 , f'폴드 {idx+1} / 폴드 {folds.n_splits}' , '#'*40)\n",
    "\n",
    "    X_train , y_train = X[train_idx] , y[train_idx] # 훈련용 데이터\n",
    "    X_valid , y_valid = X[valid_idx] , y[valid_idx] # 검증용 데이터\n",
    "\n",
    "    # LightGBM 전용 데이터셋 생성\n",
    "    dtrain = lgb.Dataset(X_train , y_train) # LightGBM 전용 훈련 데이터셋\n",
    "    dvalid = lgb.Dataset(X_valid , y_valid) # LightGBM 전용 검증 데이터셋\n",
    "\n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params = max_params , # 최적 하이퍼파라미터\n",
    "                          train_set = dtrain, # 훈련 데이터 셋\n",
    "                          num_boost_round= 2500, # 부스팅 반복 횟수\n",
    "                          valid_sets= dvalid , # 성능 평가용 검증 데이터셋\n",
    "                          feval = gini, # 검증용 평가지표\n",
    "                          early_stopping_rounds= 300, # 조기종료 조건\n",
    "                          verbose_eval = 100) # 100 번째 마다 점수 출력\n",
    "\n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "    oof_test_preds += lgb_model.predict(X_test) / folds.n_splits\n",
    "\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "    oof_test_preds_lgb = oof_test_preds\n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1}  지니계수 : {gini_score}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "(595212,)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "(595212,)"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_val_preds.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF 검증 데이터 지니계수 : 0.3949215102693418\n"
     ]
    }
   ],
   "source": [
    "print('OOF 검증 데이터 지니계수 :' , eval_gini(y, oof_val_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost 피처 엔지니어링"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# LightGBM 용 gini() 함수\n",
    "def gini(preds , dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini' , eval_gini(labels , preds) , True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# XGBoost용 gini() 함수\n",
    "\n",
    "def gini(preds , dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini' , eval_gini(labels,preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 하이퍼파라미터 최적화"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 데이터 셋 준비"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8:2 비율로 훈련 데이터 , 검증 데이터 분리( 베이지안 최적화 수행용)\n",
    "\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X,y, test_size=0.2 , random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "\n",
    "bayes_dtrain = xgb.DMatrix(X_train , y_train)\n",
    "bayes_dvalid = xgb.DMatrix(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 하이퍼파라미터 범위 설정"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'max_depth' : (4 , 8) , # 개별 트리의 최대 깊이, 트리 깊이가 깊을수록 모델이 복잡해지고 과대적합 우려\n",
    "                # 값이 클수록 깊이가 한 단계만 늘어나도 메모리 사용량이 급격히 많아진다.\n",
    "                # 일반적으로 3~10 사이의 값을 주로 사용한다.\n",
    "\n",
    "                'subsample' : (0.6 , 0.9), # 개별 트리를 훈련할 때 사용할 데이터 샘플링 비율\n",
    "                # 0~1 사이 값으로 설정할 수 있다.\n",
    "                # 0.5 로 설정하면 전체 데이터의 50%를 사용해 트리를 생성\n",
    "\n",
    "                'colsample_bytree' : (0.7 , 1.0), # 개별 트리를 훈련할 때 사용하는 피처 샘플링 비율\n",
    "                # subsample 과 유사한 개념, subsample은 전체 데이터에서 얼마나 샘플링할지 나타내는 비율\n",
    "                # colsample_bytree는 전체 피처에서 얼마나 샘플링할지 나타내는 비율\n",
    "                # 값이 작을수록 과대적합 방지 효과\n",
    "\n",
    "                'min_child_weight' : (5 , 7), # 과대적합 방지위한 값, 값이 클수록 과대적합 방지 효과가 있다.\n",
    "                'gamma' : (8 , 11), # 말단 노드가 분할하기 위한 최소 손실 감소 값\n",
    "                # 소실 감소가 gamma보다 크면 말단 노드를 분할\n",
    "                # 값이 클수록 과대적합 방지 효과가 있다.\n",
    "\n",
    "                'reg_alpha' : (7 , 9) , # L1 규제 조정 값 , 값이 클수록 과대적합 방지 효과\n",
    "                'reg_lambda' : (1.1 , 1.5), # L2 규제 조정값 , 값이 클수록 과대적합 방지 효과\n",
    "                'scale_pos_weight' : (1.4 , 1.6)} # 뷸균형 데이터 가중치 조정 값 ,\n",
    "                # 타깃값이 불균형할 때 양성 값에 scale_pos_weight 만큼 가중치를 줘서 균형을 맞춤(타깃값 1을 양성 값으로 간주)\n",
    "                # 일반적으로 scale_pos_weight 값을 (음성 타깃값 개수 / 양성 타깃값 개수) 로 설정\n",
    "\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "\n",
    "fixed_params = {'objective' : 'binary:logistic' ,# 훈련 목적 , binary : logistic( 확률값을 구하는 이진분류)\n",
    "                # reg : squarederror (회귀 문제)\n",
    "                # 소프트맥스 함수를 사용하는 다중분류에서는 multi : softmax 사용\n",
    "                # 확률값을 구하는 다중분류에서는 'multi : softprob' 사용\n",
    "                'learning_rate' : 0.02, # 학습률( 부스팅 스텝을 반복하면서 모델을 업데이트하는 데 사용되는 비율)\n",
    "                'random_state' : 1991} # 랜덤 시드값(코드를 반복 실행해도 같은 결과가 나오게 지정하는 값)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def eval_function(max_depth , subsample , colsample_bytree , min_child_weight , reg_alpha , gamma , reg_lambda , scale_pos_weight) :\n",
    "\n",
    "    # 최적화하려는 평가지표(지니계수) 계산 함수\n",
    "\n",
    "    # 베이지안 최적화를 수행할 하이퍼파라미터\n",
    "\n",
    "    params = {'max_depth' : int(round(max_depth)) , # 개별 트리의 최대깊이\n",
    "              'subsample' : subsample, # 개별 트리를 훈련할 때 사용할 데이터 샘플링 비율\n",
    "              'colsample_bytree' : colsample_bytree , # 개별 트리를 훈련할때 사용하는 피처 샘플링\n",
    "              'min_child_weight' :  # 과대적합 방지위한 값\n",
    "              min_child_weight,\n",
    "              'gamma' : gamma, # 말단 노드가 분할하기 위한 최소 손실 감소 값\n",
    "              'reg_alpha' : reg_alpha, # L1 규제 조정값\n",
    "              'reg_lambda' : reg_lambda, # L2 규제 조정값\n",
    "              'scale_pos_weight' : scale_pos_weight} # 불균형 데이터 가중치 조정값\n",
    "\n",
    "    # 값이 고정된 하이퍼파라미터도 추가\n",
    "    params.update(fixed_params)\n",
    "\n",
    "    print('하이퍼파라미터 : ' , params)\n",
    "\n",
    "    # XGBoost 모델 훈련 , train() 메서드의 하이퍼파라미터\n",
    "    xgb_model = xgb.train(params = params , # XGBoost 모델의 하이퍼파라미터 목록 , 딕셔너리 타입으로 전달\n",
    "                          dtrain = bayes_dtrain, # 훈련 데이터셋, xgboost.DMatrix 타입으로 전달\n",
    "                          num_boost_round= 2000, # 부스팅 반복 횟수, 정수형 타입으로 전달\n",
    "                          # num_boost_round 값이 클수록 성능이 좋아질 수 있으나 과대적합의 우려가 있다.\n",
    "                          # num_boost_round 값이 작으면 반복 횟수가 줄어들어 훈련 시간이 짧아진다.\n",
    "                          # 일반적으로 num_boost_round를 늘리면 learning_rate를 줄여야 한다.\n",
    "\n",
    "                          evals = [(bayes_dvalid , ' bayes_dvalid')],\n",
    "                          # 모델 성능 평가용 검증 데이터셋\n",
    "                          # (DMatrix, 문자열) 쌍들을 원소로 갖는 리스트 타입으로 전달, 검증 데이터셋 이름을 원하는 대로 문자열로 정하면 된다.\n",
    "                          maximize = True, # feval 평가지수가 높으면 좋은지 여부\n",
    "                          feval = gini, # 검증용 평가지표, 사용자 정의 함수 형태\n",
    "                          # evals를 활용해 모델 성능을 검증할 때 사용할 사용자 정의 평가지표 함수\n",
    "                          # 예측값과 실제값을 파라미터로 전달받아, 평가지표명과 평가점수를 반환하는 함수이다.\n",
    "                          early_stopping_rounds= 200,\n",
    "                          # 조기종료 조건\n",
    "                          # 모델은 기본적으로 num_boost_round만큼 훈련을 반복하며, 매 이터레이션마다 evals로 모델 성능을 평가하여 성능이 연속으로\n",
    "                          # 좋아지지 않는다면 훈련을 중단하는데, 훈련 중단에 필요한 최소횟수가 early_stopping_rounds 이다. 즉 , early_stopping_rounds\n",
    "                          # 동안 모델 성능이 좋아지지 않는다면 훈련을 중단한다.\n",
    "\n",
    "                          # 과대적합 방지 효과\n",
    "\n",
    "                          # 조기종료를 적용하기 위해서는 evals 에 검증 데이터가 하나 이상 있어야한다. 또한 evals에 검증 데이터가 여러 개라면 마지막 검증\n",
    "                          # 데이터를 기준으로 조기종료 조건을 적용한다.\n",
    "\n",
    "\n",
    "                          verbose_eval= False) # 성능 점수 로그 설정 값\n",
    "                            # True 로 설정하면 매 부스팅 스텝마다 평가점수르 출력\n",
    "                            # 출력값이 너무 많아지는 것을 방지하기위해 verbose_eval로 설정\n",
    "\n",
    "    best_iter = xgb_model.best_iteration # 최적 반복횟수\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = xgb_model.predict(bayes_dvalid , iteration_range=(0, best_iter))\n",
    "\n",
    "    # 지니계수 계산\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "\n",
    "    return gini_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 최적화 수행"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth | min_ch... | reg_alpha | reg_la... | scale_... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터 :  {'max_depth': 6, 'subsample': 0.867531900234624, 'colsample_bytree': 0.8646440511781974, 'min_child_weight': 6.0897663659937935, 'gamma': 10.14556809911726, 'reg_alpha': 7.84730959867781, 'reg_lambda': 1.3583576452266626, 'scale_pos_weight': 1.4875174422525386, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.2852804659784522\n",
      "\n",
      "| \u001B[0m1        \u001B[0m | \u001B[0m0.2853   \u001B[0m | \u001B[0m0.8646   \u001B[0m | \u001B[0m10.15    \u001B[0m | \u001B[0m6.411    \u001B[0m | \u001B[0m6.09     \u001B[0m | \u001B[0m7.847    \u001B[0m | \u001B[0m1.358    \u001B[0m | \u001B[0m1.488    \u001B[0m | \u001B[0m0.8675   \u001B[0m |\n",
      "하이퍼파라미터 :  {'max_depth': 7, 'subsample': 0.6261387899104622, 'colsample_bytree': 0.9890988281503088, 'min_child_weight': 6.0577898395058085, 'gamma': 9.150324556477333, 'reg_alpha': 8.136089122187865, 'reg_lambda': 1.4702386553170643, 'scale_pos_weight': 1.4142072116395774, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n",
      "지니계수 : 0.2848088779310762\n",
      "\n",
      "| \u001B[0m2        \u001B[0m | \u001B[0m0.2848   \u001B[0m | \u001B[0m0.9891   \u001B[0m | \u001B[0m9.15     \u001B[0m | \u001B[0m7.167    \u001B[0m | \u001B[0m6.058    \u001B[0m | \u001B[0m8.136    \u001B[0m | \u001B[0m1.47     \u001B[0m | \u001B[0m1.414    \u001B[0m | \u001B[0m0.6261   \u001B[0m |\n",
      "하이퍼파라미터 :  {'max_depth': 7, 'subsample': 0.8341587528859367, 'colsample_bytree': 0.7060655192320977, 'min_child_weight': 6.7400242964936385, 'gamma': 10.497859536643814, 'reg_alpha': 8.957236684465528, 'reg_lambda': 1.4196634256866894, 'scale_pos_weight': 1.4922958724505864, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n",
      "지니계수 : 0.28504908755804076\n",
      "\n",
      "| \u001B[0m3        \u001B[0m | \u001B[0m0.285    \u001B[0m | \u001B[0m0.7061   \u001B[0m | \u001B[0m10.5     \u001B[0m | \u001B[0m7.113    \u001B[0m | \u001B[0m6.74     \u001B[0m | \u001B[0m8.957    \u001B[0m | \u001B[0m1.42     \u001B[0m | \u001B[0m1.492    \u001B[0m | \u001B[0m0.8342   \u001B[0m |\n",
      "하이퍼파라미터 :  {'max_depth': 7, 'subsample': 0.7001630536555632, 'colsample_bytree': 0.8843124587484356, 'min_child_weight': 6.494091293383359, 'gamma': 10.452246227672624, 'reg_alpha': 8.551838810159788, 'reg_lambda': 1.3814765995549108, 'scale_pos_weight': 1.423280772455086, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.28372532851421195\n",
      "\n",
      "| \u001B[0m4        \u001B[0m | \u001B[0m0.2837   \u001B[0m | \u001B[0m0.8843   \u001B[0m | \u001B[0m10.45    \u001B[0m | \u001B[0m6.838    \u001B[0m | \u001B[0m6.494    \u001B[0m | \u001B[0m8.552    \u001B[0m | \u001B[0m1.381    \u001B[0m | \u001B[0m1.423    \u001B[0m | \u001B[0m0.7002   \u001B[0m |\n",
      "하이퍼파라미터 :  {'max_depth': 7, 'subsample': 0.8535233675350644, 'colsample_bytree': 0.92975858050776, 'min_child_weight': 6.249564429359247, 'gamma': 9.95563546750357, 'reg_alpha': 8.411512219837842, 'reg_lambda': 1.424460008293778, 'scale_pos_weight': 1.5416807226581535, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.28530004505451917\n",
      "\n",
      "| \u001B[95m5        \u001B[0m | \u001B[95m0.2853   \u001B[0m | \u001B[95m0.9298   \u001B[0m | \u001B[95m9.956    \u001B[0m | \u001B[95m6.809    \u001B[0m | \u001B[95m6.25     \u001B[0m | \u001B[95m8.412    \u001B[0m | \u001B[95m1.424    \u001B[0m | \u001B[95m1.542    \u001B[0m | \u001B[95m0.8535   \u001B[0m |\n",
      "하이퍼파라미터 :  {'max_depth': 7, 'subsample': 0.6462619019069298, 'colsample_bytree': 0.80929192865947, 'min_child_weight': 6.079999276892042, 'gamma': 9.553916776586505, 'reg_alpha': 8.860396362258099, 'reg_lambda': 1.4050740023119348, 'scale_pos_weight': 1.4668544695338273, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.2852898511572112\n",
      "\n",
      "| \u001B[0m6        \u001B[0m | \u001B[0m0.2853   \u001B[0m | \u001B[0m0.8093   \u001B[0m | \u001B[0m9.554    \u001B[0m | \u001B[0m6.532    \u001B[0m | \u001B[0m6.08     \u001B[0m | \u001B[0m8.86     \u001B[0m | \u001B[0m1.405    \u001B[0m | \u001B[0m1.467    \u001B[0m | \u001B[0m0.6463   \u001B[0m |\n",
      "하이퍼파라미터 :  {'max_depth': 7, 'subsample': 0.6931141936797243, 'colsample_bytree': 0.8817801730078565, 'min_child_weight': 6.992334203641873, 'gamma': 9.013424730095146, 'reg_alpha': 7.640858389939128, 'reg_lambda': 1.3562805915715632, 'scale_pos_weight': 1.449446257931491, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.28467572646560496\n",
      "\n",
      "| \u001B[0m7        \u001B[0m | \u001B[0m0.2847   \u001B[0m | \u001B[0m0.8818   \u001B[0m | \u001B[0m9.013    \u001B[0m | \u001B[0m6.927    \u001B[0m | \u001B[0m6.992    \u001B[0m | \u001B[0m7.641    \u001B[0m | \u001B[0m1.356    \u001B[0m | \u001B[0m1.449    \u001B[0m | \u001B[0m0.6931   \u001B[0m |\n",
      "하이퍼파라미터 :  {'max_depth': 5, 'subsample': 0.6261564417044092, 'colsample_bytree': 0.8763145220620449, 'min_child_weight': 5.135323353557588, 'gamma': 8.39495450163982, 'reg_alpha': 8.950443047087845, 'reg_lambda': 1.4235649099168255, 'scale_pos_weight': 1.5217625173811569, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.28457181017567373\n",
      "\n",
      "| \u001B[0m8        \u001B[0m | \u001B[0m0.2846   \u001B[0m | \u001B[0m0.8763   \u001B[0m | \u001B[0m8.395    \u001B[0m | \u001B[0m4.561    \u001B[0m | \u001B[0m5.135    \u001B[0m | \u001B[0m8.95     \u001B[0m | \u001B[0m1.424    \u001B[0m | \u001B[0m1.522    \u001B[0m | \u001B[0m0.6262   \u001B[0m |\n",
      "하이퍼파라미터 :  {'max_depth': 6, 'subsample': 0.857971740304964, 'colsample_bytree': 0.9583821245229369, 'min_child_weight': 6.158305055403563, 'gamma': 9.305332775334449, 'reg_alpha': 8.200928434091152, 'reg_lambda': 1.2571039588093065, 'scale_pos_weight': 1.4700266933495618, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지니계수 : 0.28538973194158385\n",
      "\n",
      "| \u001B[95m9        \u001B[0m | \u001B[95m0.2854   \u001B[0m | \u001B[95m0.9584   \u001B[0m | \u001B[95m9.305    \u001B[0m | \u001B[95m5.594    \u001B[0m | \u001B[95m6.158    \u001B[0m | \u001B[95m8.201    \u001B[0m | \u001B[95m1.257    \u001B[0m | \u001B[95m1.47     \u001B[0m | \u001B[95m0.858    \u001B[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "\n",
    "optimizer = BayesianOptimization(f= eval_function, pbounds = param_bounds , random_state= 0)\n",
    "\n",
    "\n",
    "# 베이지안 최적화 수행\n",
    "\n",
    "\n",
    "optimizer.maximize(init_points= 3 , n_iter= 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 결과확인"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "{'colsample_bytree': 0.9583821245229369,\n 'gamma': 9.305332775334449,\n 'max_depth': 5.594282602920541,\n 'min_child_weight': 6.158305055403563,\n 'reg_alpha': 8.200928434091152,\n 'reg_lambda': 1.2571039588093065,\n 'scale_pos_weight': 1.4700266933495618,\n 'subsample': 0.857971740304964}"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가함수 점수가 최대일 때 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "{'colsample_bytree': 0.9583821245229369,\n 'gamma': 9.305332775334449,\n 'max_depth': 6,\n 'min_child_weight': 6.158305055403563,\n 'reg_alpha': 8.200928434091152,\n 'reg_lambda': 1.2571039588093065,\n 'scale_pos_weight': 1.4700266933495618,\n 'subsample': 0.857971740304964,\n 'objective': 'binary:logistic',\n 'learning_rate': 0.02,\n 'random_state': 1991}"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수형 하이퍼파라미터 변환\n",
    "\n",
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터 추가\n",
    "\n",
    "max_params.update(fixed_params)\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 훈련 및 성능 검증"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\xgboost\\training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalid-logloss:0.67671\tvalid-gini:0.16215\n",
      "[100]\tvalid-logloss:0.19182\tvalid-gini:0.24941\n",
      "[200]\tvalid-logloss:0.15837\tvalid-gini:0.27811\n",
      "[300]\tvalid-logloss:0.15506\tvalid-gini:0.28780\n",
      "[400]\tvalid-logloss:0.15452\tvalid-gini:0.29319\n",
      "[500]\tvalid-logloss:0.15439\tvalid-gini:0.29533\n",
      "[600]\tvalid-logloss:0.15433\tvalid-gini:0.29681\n",
      "[700]\tvalid-logloss:0.15427\tvalid-gini:0.29816\n",
      "[800]\tvalid-logloss:0.15425\tvalid-gini:0.29872\n",
      "[900]\tvalid-logloss:0.15422\tvalid-gini:0.29916\n",
      "[1000]\tvalid-logloss:0.15422\tvalid-gini:0.29949\n",
      "[1100]\tvalid-logloss:0.15422\tvalid-gini:0.29951\n",
      "[1200]\tvalid-logloss:0.15422\tvalid-gini:0.29941\n",
      "[1300]\tvalid-logloss:0.15420\tvalid-gini:0.29947\n",
      "[1337]\tvalid-logloss:0.15421\tvalid-gini:0.29944\n",
      "폴드 1  지니계수 : 0.2996383794197772\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.67670\tvalid-gini:0.15338\n",
      "[100]\tvalid-logloss:0.19190\tvalid-gini:0.24056\n",
      "[200]\tvalid-logloss:0.15862\tvalid-gini:0.26445\n",
      "[300]\tvalid-logloss:0.15542\tvalid-gini:0.27408\n",
      "[400]\tvalid-logloss:0.15493\tvalid-gini:0.27824\n",
      "[500]\tvalid-logloss:0.15479\tvalid-gini:0.28058\n",
      "[600]\tvalid-logloss:0.15474\tvalid-gini:0.28193\n",
      "[700]\tvalid-logloss:0.15471\tvalid-gini:0.28279\n",
      "[800]\tvalid-logloss:0.15471\tvalid-gini:0.28313\n",
      "[900]\tvalid-logloss:0.15468\tvalid-gini:0.28360\n",
      "[1000]\tvalid-logloss:0.15467\tvalid-gini:0.28425\n",
      "[1100]\tvalid-logloss:0.15467\tvalid-gini:0.28435\n",
      "[1200]\tvalid-logloss:0.15465\tvalid-gini:0.28475\n",
      "[1300]\tvalid-logloss:0.15465\tvalid-gini:0.28490\n",
      "[1400]\tvalid-logloss:0.15464\tvalid-gini:0.28515\n",
      "[1500]\tvalid-logloss:0.15464\tvalid-gini:0.28503\n",
      "[1600]\tvalid-logloss:0.15465\tvalid-gini:0.28503\n",
      "[1635]\tvalid-logloss:0.15464\tvalid-gini:0.28506\n",
      "폴드 2  지니계수 : 0.2852264715520086\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.67670\tvalid-gini:0.15662\n",
      "[100]\tvalid-logloss:0.19179\tvalid-gini:0.24702\n",
      "[200]\tvalid-logloss:0.15838\tvalid-gini:0.27073\n",
      "[300]\tvalid-logloss:0.15509\tvalid-gini:0.27895\n",
      "[400]\tvalid-logloss:0.15462\tvalid-gini:0.28197\n",
      "[500]\tvalid-logloss:0.15454\tvalid-gini:0.28295\n",
      "[600]\tvalid-logloss:0.15452\tvalid-gini:0.28350\n",
      "[700]\tvalid-logloss:0.15451\tvalid-gini:0.28375\n",
      "[800]\tvalid-logloss:0.15447\tvalid-gini:0.28444\n",
      "[900]\tvalid-logloss:0.15448\tvalid-gini:0.28427\n",
      "[1000]\tvalid-logloss:0.15448\tvalid-gini:0.28452\n",
      "[1100]\tvalid-logloss:0.15448\tvalid-gini:0.28462\n",
      "[1200]\tvalid-logloss:0.15448\tvalid-gini:0.28461\n",
      "[1300]\tvalid-logloss:0.15449\tvalid-gini:0.28438\n",
      "[1364]\tvalid-logloss:0.15447\tvalid-gini:0.28467\n",
      "폴드 3  지니계수 : 0.28469150190156506\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.67670\tvalid-gini:0.16821\n",
      "[100]\tvalid-logloss:0.19173\tvalid-gini:0.23869\n",
      "[200]\tvalid-logloss:0.15846\tvalid-gini:0.26465\n",
      "[300]\tvalid-logloss:0.15526\tvalid-gini:0.27270\n",
      "[400]\tvalid-logloss:0.15482\tvalid-gini:0.27515\n",
      "[500]\tvalid-logloss:0.15472\tvalid-gini:0.27660\n",
      "[600]\tvalid-logloss:0.15467\tvalid-gini:0.27774\n",
      "[700]\tvalid-logloss:0.15464\tvalid-gini:0.27846\n",
      "[800]\tvalid-logloss:0.15462\tvalid-gini:0.27907\n",
      "[900]\tvalid-logloss:0.15461\tvalid-gini:0.27949\n",
      "[1000]\tvalid-logloss:0.15459\tvalid-gini:0.27973\n",
      "[1100]\tvalid-logloss:0.15460\tvalid-gini:0.27975\n",
      "[1200]\tvalid-logloss:0.15460\tvalid-gini:0.27984\n",
      "[1300]\tvalid-logloss:0.15460\tvalid-gini:0.27984\n",
      "[1400]\tvalid-logloss:0.15460\tvalid-gini:0.27981\n",
      "[1425]\tvalid-logloss:0.15461\tvalid-gini:0.27974\n",
      "폴드 4  지니계수 : 0.27991724318908323\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[0]\tvalid-logloss:0.67671\tvalid-gini:0.17061\n",
      "[100]\tvalid-logloss:0.19185\tvalid-gini:0.24638\n",
      "[200]\tvalid-logloss:0.15860\tvalid-gini:0.27270\n",
      "[300]\tvalid-logloss:0.15535\tvalid-gini:0.28310\n",
      "[400]\tvalid-logloss:0.15484\tvalid-gini:0.28760\n",
      "[500]\tvalid-logloss:0.15469\tvalid-gini:0.29034\n",
      "[600]\tvalid-logloss:0.15462\tvalid-gini:0.29223\n",
      "[700]\tvalid-logloss:0.15458\tvalid-gini:0.29313\n",
      "[800]\tvalid-logloss:0.15455\tvalid-gini:0.29412\n",
      "[900]\tvalid-logloss:0.15453\tvalid-gini:0.29438\n",
      "[1000]\tvalid-logloss:0.15452\tvalid-gini:0.29502\n",
      "[1100]\tvalid-logloss:0.15451\tvalid-gini:0.29513\n",
      "[1200]\tvalid-logloss:0.15448\tvalid-gini:0.29535\n",
      "[1300]\tvalid-logloss:0.15449\tvalid-gini:0.29533\n",
      "[1400]\tvalid-logloss:0.15447\tvalid-gini:0.29586\n",
      "[1500]\tvalid-logloss:0.15446\tvalid-gini:0.29596\n",
      "[1600]\tvalid-logloss:0.15446\tvalid-gini:0.29603\n",
      "[1700]\tvalid-logloss:0.15445\tvalid-gini:0.29610\n",
      "[1800]\tvalid-logloss:0.15444\tvalid-gini:0.29622\n",
      "[1900]\tvalid-logloss:0.15445\tvalid-gini:0.29619\n",
      "[1999]\tvalid-logloss:0.15444\tvalid-gini:0.29641\n",
      "폴드 5  지니계수 : 0.2964012751575175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기 생성\n",
    "folds = StratifiedKFold(n_splits= 5 , shuffle= True , random_state= 1991)\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "\n",
    "# OOF 방식으로 훈련된 모델 훈련 , 검증 , 예측\n",
    "\n",
    "for idx , (train_idx , valid_idx) in enumerate(folds.split(X,y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "\n",
    "    print('#' *40,  f'폴드 {idx+1} / 폴드 {folds.n_splits}' , '#'*40)\n",
    "\n",
    "\n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train , y_train = X[train_idx] , y[train_idx]\n",
    "    X_valid , y_valid = X[valid_idx] , y[valid_idx]\n",
    "\n",
    "    #XGBoost 전용 데이터셋 생성\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train , y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid , y_valid)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    #XGBoost 모델 훈련\n",
    "    xgb_model = xgb.train(params = max_params,\n",
    "                          dtrain = dtrain,\n",
    "                          num_boost_round = 2000,\n",
    "                          evals = [(dvalid , 'valid')],\n",
    "                          maximize = True,\n",
    "                          feval = gini,\n",
    "                          early_stopping_rounds = 200,\n",
    "                          verbose_eval = 100)\n",
    "\n",
    "    # 모델 성능이 가장 좋을 때의 부스팅 반복 횟수 저장\n",
    "    best_iter= xgb_model.best_iteration\n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "\n",
    "    oof_test_preds += xgb_model.predict(dtest, iteration_range = (0 , best_iter))/ folds.n_splits\n",
    "\n",
    "    oof_test_preds_xgb = oof_test_preds\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "    oof_val_preds[valid_idx] += xgb_model.predict(dvalid , iteration_range=(0, best_iter))\n",
    "\n",
    "\n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "    gini_score = eval_gini(y_valid , oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1}  지니계수 : {gini_score}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oof_val_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-90ae067261ef>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0moof_val_preds\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'oof_val_preds' is not defined"
     ]
    }
   ],
   "source": [
    "oof_val_preds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF 검증 데이터 지니계수 :  0.28905038063620453\n"
     ]
    }
   ],
   "source": [
    "print('OOF 검증 데이터 지니계수 : ' , eval_gini(y , oof_val_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 성능 개선 3 : LightGBM 과 XGBoost 앙상블"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "oof_test_preds = oof_test_preds_lgb *0.5 + oof_test_preds_xgb * 0.5\n",
    "\n",
    "\n",
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "           target\nid               \n0        0.069984\n1        0.055953\n2        0.059893\n3        0.036767\n4        0.092453\n...           ...\n1488022  0.210374\n1488023  0.108345\n1488024  0.084375\n1488025  0.050856\n1488026  0.075178\n\n[892816 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.069984</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.055953</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.059893</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.036767</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.092453</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1488022</th>\n      <td>0.210374</td>\n    </tr>\n    <tr>\n      <th>1488023</th>\n      <td>0.108345</td>\n    </tr>\n    <tr>\n      <th>1488024</th>\n      <td>0.084375</td>\n    </tr>\n    <tr>\n      <th>1488025</th>\n      <td>0.050856</td>\n    </tr>\n    <tr>\n      <th>1488026</th>\n      <td>0.075178</td>\n    </tr>\n  </tbody>\n</table>\n<p>892816 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}