{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 경로\n",
    "\n",
    "data_path = './porto-seguro-safe-driver-prediction/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv' , index_col = 'id')\n",
    "test = pd.read_csv(data_path + 'test.csv' , index_col = 'id')\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv' , index_col ='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "         ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n0                2              2          5              1              0   \n1                1              1          7              0              0   \n2                5              4          9              1              0   \n3                0              1          2              0              0   \n4                0              2          0              1              0   \n...            ...            ...        ...            ...            ...   \n1488023          0              1          6              0              0   \n1488024          5              3          5              1              0   \n1488025          0              1          5              0              0   \n1488026          6              1          5              1              0   \n1488027          7              1          4              1              0   \n\n         ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  \\\n0                    0              1              0              0   \n1                    0              0              1              0   \n2                    0              0              1              0   \n3                    1              0              0              0   \n4                    1              0              0              0   \n...                ...            ...            ...            ...   \n1488023              0              1              0              0   \n1488024              0              0              1              0   \n1488025              1              0              0              0   \n1488026              0              0              0              1   \n1488027              0              0              0              1   \n\n         ps_ind_10_bin  ...  ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  \\\n0                    0  ...           9           1           5           8   \n1                    0  ...           3           1           1           9   \n2                    0  ...           4           2           7           7   \n3                    0  ...           2           2           4           9   \n4                    0  ...           3           1           1           3   \n...                ...  ...         ...         ...         ...         ...   \n1488023              0  ...           4           2           3           4   \n1488024              0  ...           6           2           2          11   \n1488025              0  ...           5           2           2          11   \n1488026              0  ...           1           1           2           7   \n1488027              0  ...           5           2           2           7   \n\n         ps_calc_15_bin  ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  \\\n0                     0               1               1               0   \n1                     0               1               1               0   \n2                     0               1               1               0   \n3                     0               0               0               0   \n4                     0               0               0               1   \n...                 ...             ...             ...             ...   \n1488023               0               1               0               0   \n1488024               0               0               1               1   \n1488025               0               1               1               0   \n1488026               1               1               0               0   \n1488027               0               1               1               1   \n\n         ps_calc_19_bin  ps_calc_20_bin  \n0                     0               1  \n1                     1               0  \n2                     1               0  \n3                     0               0  \n4                     1               0  \n...                 ...             ...  \n1488023               1               0  \n1488024               0               0  \n1488025               0               0  \n1488026               0               0  \n1488027               0               0  \n\n[1488028 rows x 57 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ps_ind_01</th>\n      <th>ps_ind_02_cat</th>\n      <th>ps_ind_03</th>\n      <th>ps_ind_04_cat</th>\n      <th>ps_ind_05_cat</th>\n      <th>ps_ind_06_bin</th>\n      <th>ps_ind_07_bin</th>\n      <th>ps_ind_08_bin</th>\n      <th>ps_ind_09_bin</th>\n      <th>ps_ind_10_bin</th>\n      <th>...</th>\n      <th>ps_calc_11</th>\n      <th>ps_calc_12</th>\n      <th>ps_calc_13</th>\n      <th>ps_calc_14</th>\n      <th>ps_calc_15_bin</th>\n      <th>ps_calc_16_bin</th>\n      <th>ps_calc_17_bin</th>\n      <th>ps_calc_18_bin</th>\n      <th>ps_calc_19_bin</th>\n      <th>ps_calc_20_bin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>5</td>\n      <td>8</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>9</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>4</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>4</td>\n      <td>2</td>\n      <td>7</td>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1488023</th>\n      <td>0</td>\n      <td>1</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>4</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1488024</th>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>2</td>\n      <td>2</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1488025</th>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2</td>\n      <td>11</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1488026</th>\n      <td>6</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>7</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1488027</th>\n      <td>7</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2</td>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1488028 rows × 57 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.concat([train , test] , ignore_index= True)\n",
    "all_data = all_data.drop('target' , axis = 1) # 타깃값 제거\n",
    "\n",
    "all_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n       'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n       'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n       'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n       'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n       'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n       'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n       'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n       'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n       'ps_calc_20_bin'],\n      dtype='object')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = all_data.columns # 전체 피처\n",
    "all_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 명목형 피처 원-핫 인코딩"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<1488028x184 sparse matrix of type '<class 'numpy.float64'>'\n\twith 20832392 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 명목형 피처 추출\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "# 이름에 cat이 포함된 피처가 명목형 피처이다.\n",
    "onehot_encoder = OneHotEncoder() # 원-핫 인코더 객체 생성\n",
    "\n",
    "# 인코딩\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])\n",
    "\n",
    "encoded_cat_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 추가로 제거할 피처\n",
    "drop_features = ['ps_ind14' , 'ps_ind_10_bin' , 'ps_ind_11_bin' , 'ps_ind_12_bin' , 'ps_ind_13_bin' , 'ps_car_14' ]\n",
    "\n",
    "# 1> 명목형 피처 , 2> calc 분류의 피처 , 3> 추가 제거할 피처를 제외한 피처\n",
    "\n",
    "remaining_features = [feature for feature in all_features\n",
    "                      if ('cat' not in feature and\n",
    "                          'calc' not in feature and\n",
    "                          feature not in drop_features)]\n",
    "# cat(명목형) 피처 , calc(피처) , 추가 제거할 6개피처를 제외한 나머지 피처를 remaining_features 에 저장"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<1488028x202 sparse matrix of type '<class 'numpy.float64'>'\n\twith 37644877 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data[remaining_features]) , encoded_cat_matrix] , format= 'csr')\n",
    "\n",
    "all_data_sprs\n",
    "\n",
    "# CSR  형식으로 바꾸어 hstack()으로 행렬을 수평 방향으로 합친다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train :]\n",
    "\n",
    "y = train['target'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 지니계수"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 지니계수란?\n",
    "\n",
    "# 소득 불평등 정도를 나타내는 지표. 지니계수가 작을수록 소득 수준이 평등하고, 클수록 불평등함을 의미한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 정규화 지니계수 계산 함수\n",
    "\n",
    "# 정규화란 값의 범위를 0~1 사이로 조정한다는 의미, 정규화 지니계수는 값이 0에 가까울수록 성능이 나쁘고, 1에 가까울 수록 성능이 좋다는 의미가 된다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_gini(y_true , y_pred):\n",
    "    # 실제값과 예측값의 크기가 서로 같은지 확인(값이 다르면 오류 발생)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    n_samples = y_true.shape[0] # 데이터 개수\n",
    "    L_mid = np.linspace(1/ n_samples ,1 , n_samples) # 대각선 값\n",
    "\n",
    "    # 1) 예측값에 대한 지니계수\n",
    "\n",
    "    pred_order = y_true[y_pred.argsort()] # y_pred 크기순으로 y_true 값 정렬\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선\n",
    "\n",
    "    G_pred = np.sum(L_mid - L_pred) # 예측값에 대한 지니계수\n",
    "\n",
    "    # 2) 예측이 완벽할 때 지니계쑤\n",
    "\n",
    "    true_order = y_true[y_true.argsort()] # y_true 크기순으로 y_true 값 정렬\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order) # 로렌츠 곡선\n",
    "    G_true = np.sum(L_mid - L_true) # 예측이 완벽할 때 지니계수\n",
    "\n",
    "    # 정규화된 지니계수\n",
    "    return G_pred / G_true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# LightGBM 용 gini() 함수\n",
    "\n",
    "def gini(preds , dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "\n",
    "    return 'gini' , eval_gini(labels , preds ) , True\n",
    "\n",
    "# 'gini' : 평가지표이름 , eval_gini(labels,preds) : 평가점수 , True : 평가 점수가 높을수록 좋은지 여부"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 훈련 및 성능 검증"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# OOF(Out of Fold prediction) 예측 방식\n",
    "\n",
    "# K 폴드 교차 검증을 수행하면서 각 폴드마다 테스트 데이터로 예측하는 방식이다.\n",
    "\n",
    "# K 폴드 교차 검ㅈ응을 하면서 폴드마다 1> 훈련 데이터로 모델을 훈련하고, 2> 검증 데이터로 모델 성능을 측정하며 , 3> 테스트 데이터로 최종 타깃 확률도 예측한다. 훈련된 모델로 마지막에 한 번만 예측하는 것이 아니다. 각 폴드별 모델로 여러번 예측해 평균을 내는 방식이다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# OOF 방식으로 LightGBM 훈련\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기\n",
    "\n",
    "folds = StratifiedKFold(n_splits= 5 , shuffle= True , random_state= 1991)\n",
    "\n",
    "# 층화 K 폴드 교차 검증기는 타깃값이 불균형하므로 K폴드가 아닌 층화 K폴드를 수행하는 게 바람직하다. 층화 K폴드는 타깃값이 균등하게\n",
    "# 폴드를 나누는 방식이기 때문이다.\n",
    "\n",
    "\n",
    "# n_splits 파라미터로 전달한 수만큼 폴드를 나눈다. 여기서는 5개로 나누었다. shuffle = True 를 전달하면 폴드를 나눌때 데이터를 섞어준다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# LightGBM의 하이퍼파라미터를 설정한다. LightGBM은 하이퍼파라미터를 갖고 있지만, 여기서는 4가지만 설정한다.\n",
    "\n",
    "params = {'objective' : 'binary' , 'learning_rate' : 0.01 , 'force_row_wise' : True , 'random_state' : 0}\n",
    "\n",
    "# 이진분류 문제이므로 objective 파라미터는 binary로 설정했다. 학습률은 0.01로, 랜덤 스테이트 값은 9으로 설정했다.\n",
    "# force_row_wise : True 는 경고 문구를 없애려고 추가한 파라미터이다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "\n",
    "# ==> oof_val_preds 는 검증 데이터를 활용해 예측한 확률값을 저장하는 배열이다. K 폴드로 나누어도 훈련 데이터 전체가 결국엔 한 번씩 검증 데이터로 활용된다. 따라서 oof_val_preds 배열 크기는 훈련 데이터와 같아야 한다.\n",
    "# 훈련 데이터 개수는 X.shpae[0]으로 구한다.\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# oof_test_preds는 테스트 데이터를 활용해 예측한 확률값을 저장하는 배열이다. 최종 제출에 사용할 값이므로 크기는 테스트 데이터와 같아야한다. 테스트 데이터 개수는 X_test.shape[0]으로 구한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# !pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org lightgbm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1100\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153356\tvalid_0's gini: 0.261373\n",
      "[200]\tvalid_0's binary_logloss: 0.152424\tvalid_0's gini: 0.27589\n",
      "[300]\tvalid_0's binary_logloss: 0.152031\tvalid_0's gini: 0.281949\n",
      "[400]\tvalid_0's binary_logloss: 0.151802\tvalid_0's gini: 0.286194\n",
      "[500]\tvalid_0's binary_logloss: 0.151737\tvalid_0's gini: 0.286881\n",
      "[600]\tvalid_0's binary_logloss: 0.151686\tvalid_0's gini: 0.287701\n",
      "[700]\tvalid_0's binary_logloss: 0.151677\tvalid_0's gini: 0.287901\n",
      "Early stopping, best iteration is:\n",
      "[655]\tvalid_0's binary_logloss: 0.151674\tvalid_0's gini: 0.287982\n",
      "폴드 1 지니계수 : 0.2879824293420261\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153494\tvalid_0's gini: 0.249939\n",
      "[200]\tvalid_0's binary_logloss: 0.152711\tvalid_0's gini: 0.260705\n",
      "[300]\tvalid_0's binary_logloss: 0.152396\tvalid_0's gini: 0.267007\n",
      "[400]\tvalid_0's binary_logloss: 0.152249\tvalid_0's gini: 0.271043\n",
      "[500]\tvalid_0's binary_logloss: 0.152185\tvalid_0's gini: 0.272626\n",
      "[600]\tvalid_0's binary_logloss: 0.152165\tvalid_0's gini: 0.273257\n",
      "[700]\tvalid_0's binary_logloss: 0.152163\tvalid_0's gini: 0.27313\n",
      "Early stopping, best iteration is:\n",
      "[653]\tvalid_0's binary_logloss: 0.152158\tvalid_0's gini: 0.273406\n",
      "폴드 2 지니계수 : 0.2734064630206154\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153263\tvalid_0's gini: 0.261144\n",
      "[200]\tvalid_0's binary_logloss: 0.152341\tvalid_0's gini: 0.271587\n",
      "[300]\tvalid_0's binary_logloss: 0.151976\tvalid_0's gini: 0.276629\n",
      "[400]\tvalid_0's binary_logloss: 0.151818\tvalid_0's gini: 0.278742\n",
      "[500]\tvalid_0's binary_logloss: 0.151759\tvalid_0's gini: 0.279933\n",
      "[600]\tvalid_0's binary_logloss: 0.151759\tvalid_0's gini: 0.280056\n",
      "Early stopping, best iteration is:\n",
      "[541]\tvalid_0's binary_logloss: 0.151753\tvalid_0's gini: 0.280206\n",
      "폴드 3 지니계수 : 0.2802057321746898\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1100\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153399\tvalid_0's gini: 0.25056\n",
      "[200]\tvalid_0's binary_logloss: 0.152558\tvalid_0's gini: 0.262678\n",
      "[300]\tvalid_0's binary_logloss: 0.152249\tvalid_0's gini: 0.267308\n",
      "[400]\tvalid_0's binary_logloss: 0.152117\tvalid_0's gini: 0.269741\n",
      "[500]\tvalid_0's binary_logloss: 0.152071\tvalid_0's gini: 0.270738\n",
      "[600]\tvalid_0's binary_logloss: 0.152061\tvalid_0's gini: 0.27121\n",
      "[700]\tvalid_0's binary_logloss: 0.152069\tvalid_0's gini: 0.27145\n",
      "Early stopping, best iteration is:\n",
      "[608]\tvalid_0's binary_logloss: 0.152061\tvalid_0's gini: 0.271241\n",
      "폴드 4 지니계수 : 0.2712407835640063\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1103\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153481\tvalid_0's gini: 0.261934\n",
      "[200]\tvalid_0's binary_logloss: 0.152645\tvalid_0's gini: 0.273354\n",
      "[300]\tvalid_0's binary_logloss: 0.152268\tvalid_0's gini: 0.280299\n",
      "[400]\tvalid_0's binary_logloss: 0.152071\tvalid_0's gini: 0.284938\n",
      "[500]\tvalid_0's binary_logloss: 0.152005\tvalid_0's gini: 0.286724\n",
      "[600]\tvalid_0's binary_logloss: 0.151986\tvalid_0's gini: 0.28717\n",
      "[700]\tvalid_0's binary_logloss: 0.151979\tvalid_0's gini: 0.287158\n",
      "Early stopping, best iteration is:\n",
      "[658]\tvalid_0's binary_logloss: 0.151973\tvalid_0's gini: 0.287407\n",
      "폴드 5 지니계수 : 0.2874066737367479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# OOF 방식으로 모델 훈련 , 검증 , 예측\n",
    "\n",
    "for idx, (train_idx , valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40 , f'폴드 {idx +1} / 폴드 {folds.n_splits}' , '#'*40)\n",
    "\n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train , y_train = X[train_idx] , y[train_idx] # 훈련용 데이터\n",
    "    X_valid , y_valid = X[valid_idx] , y[valid_idx] # 검증용 데이터\n",
    "\n",
    "    # LightGBM 전용 데이터셋 생성\n",
    "    dtrain = lgb.Dataset(X_train , y_train) # LightGBM 전용 훈련 데이터 셋\n",
    "    dvalid = lgb.Dataset(X_valid , y_valid) # LightGBM 전용 검증 데이터 셋\n",
    "\n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params = params , # 훈련용 하이퍼파라미터\n",
    "                          train_set = dtrain, # 훈련 데이터 셋\n",
    "                          num_boost_round = 1000, # 부스팅 반복 횟수\n",
    "                          valid_sets=  dvalid ,  # 성능 평가용 검증 데이터 셋\n",
    "                          feval = gini, # 검증용 평가지표\n",
    "                          early_stopping_rounds = 100, # 조기종료 조건\n",
    "                          verbose_eval = 100 ) # 100번째마다 점수 출력\n",
    "\n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "\n",
    "    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n",
    "\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "\n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "\n",
    "    gini_score = eval_gini(y_valid , oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx +1} 지니계수 : {gini_score}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF 검증 데이터 지니계수 :  0.27992426994281666\n"
     ]
    }
   ],
   "source": [
    "print('OOF 검증 데이터 지니계수 : ' , eval_gini(y , oof_val_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 성능 개선"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n       'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n       'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n       'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n       'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n       'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n       'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n       'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n       'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n       'ps_calc_20_bin'],\n      dtype='object')"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.concat([train , test] , ignore_index = True)\n",
    "all_data = all_data.drop('target' , axis =1 ) # 타깃값 제거\n",
    "\n",
    "all_features = all_data.columns # 전체 피처\n",
    "\n",
    "all_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 명목형 피처\n",
    "\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "\n",
    "# 원-핫 인코딩 적용\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 파생 피처 추가"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# 탐색적 데이터 분석과정에서는 필요 없는 피처를 추리는 것 외에 특별한 피처 엔지니어링 건을 찾아내지 못했다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# 첫번째 , 한 데이터가 가진 결측값 개수를 파생 피처로 만들어본다. -1 이 결측값이므로 결측값 개수를 구하려면 -1 개수를 구하면 된다.\n",
    "\n",
    "# 데이터 하나당 결측값 개수를 파생 피처로 추가\n",
    "\n",
    "all_data['num_missing'] = (all_data == -1).sum(axis = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "['ps_ind_01',\n 'ps_ind_03',\n 'ps_ind_06_bin',\n 'ps_ind_07_bin',\n 'ps_ind_08_bin',\n 'ps_ind_09_bin',\n 'ps_ind_10_bin',\n 'ps_ind_11_bin',\n 'ps_ind_12_bin',\n 'ps_ind_13_bin',\n 'ps_ind_14',\n 'ps_ind_15',\n 'ps_ind_16_bin',\n 'ps_ind_17_bin',\n 'ps_ind_18_bin',\n 'ps_reg_01',\n 'ps_reg_02',\n 'ps_reg_03',\n 'ps_car_11',\n 'ps_car_12',\n 'ps_car_13',\n 'ps_car_14',\n 'ps_car_15',\n 'num_missing']"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명목형 피처 , calc 분류의 피처를 제외한 피처\n",
    "\n",
    "remaining_features = [feature for feature in all_features if ('cat' not in feature and 'calc' not in feature)]\n",
    "\n",
    "# num_missing을 remaining_features 에 추가\n",
    "\n",
    "remaining_features.append('num_missing')\n",
    "\n",
    "remaining_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# 두번째, ind 분류의 피처들을 살펴본다. 모든 ind 피처 값을 연결해서 새로운 피처를 만들려고한다.\n",
    "\n",
    "# 예를들어 , ps_ind_01 , ps_ind_02_car , ps_ind_03의 값이 각각 2, 3, 5 라면 모든 값을 연결해 2_2_5_로 만든다.\n",
    "\n",
    "# ind 피처가 총 18개이므로 18개 값이 연결된다.\n",
    "\n",
    "ind_features = [feature for feature in all_features if 'ind' in feature]\n",
    "\n",
    "is_first_feature = True\n",
    "\n",
    "for ind_feature in ind_features:\n",
    "    if is_first_feature:\n",
    "        all_data['mix_ind'] = all_data[ind_feature].astype(str) + '-'\n",
    "        is_first_feature = False\n",
    "    else:\n",
    "        all_data['mix_ind'] += all_data[ind_feature].astype(str) + '-'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "0          2-2-5-1-0-0-1-0-0-0-0-0-0-0-11-0-1-0-\n1           1-1-7-0-0-0-0-1-0-0-0-0-0-0-3-0-0-1-\n2          5-4-9-1-0-0-0-1-0-0-0-0-0-0-12-1-0-0-\n3           0-1-2-0-0-1-0-0-0-0-0-0-0-0-8-1-0-0-\n4           0-2-0-1-0-1-0-0-0-0-0-0-0-0-9-1-0-0-\n                           ...                  \n1488023     0-1-6-0-0-0-1-0-0-0-0-0-0-0-2-0-0-1-\n1488024    5-3-5-1-0-0-0-1-0-0-0-0-0-0-11-1-0-0-\n1488025     0-1-5-0-0-1-0-0-0-0-0-0-0-0-5-0-0-1-\n1488026    6-1-5-1-0-0-0-0-1-0-0-0-0-0-13-1-0-0-\n1488027    7-1-4-1-0-0-0-0-1-0-0-0-0-0-12-1-0-0-\nName: mix_ind, Length: 1488028, dtype: object"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['mix_ind']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": " 1    1079327\n 2     309747\n 3      70172\n 4      28259\n-1        523\nName: ps_ind_02_cat, dtype: int64"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 세번째 , 명목형 피처의 고윳값별 개수를 새로운 피처로 추가한다. 고윳값별 개수는 value_counts()로 구한다.\n",
    "\n",
    "# ps_ind_02_cat 피처의 고윳값별 개수 코드\n",
    "\n",
    "all_data['ps_ind_02_cat'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "{1: 1079327, 2: 309747, 3: 70172, 4: 28259, -1: 523}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명목형 피처의 고윳값별 개수를 파생 피처로 생성\n",
    "all_data['ps_ind_02_cat'].value_counts().to_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "cat_count_features = []\n",
    "\n",
    "for feature in cat_features+['mix_ind']:\n",
    "    val_counts_dict = all_data[feature].value_counts().to_dict()\n",
    "    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x: val_counts_dict[x])\n",
    "\n",
    "    cat_count_features.append(f'{feature}_count')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "['ps_ind_02_cat_count',\n 'ps_ind_04_cat_count',\n 'ps_ind_05_cat_count',\n 'ps_car_01_cat_count',\n 'ps_car_02_cat_count',\n 'ps_car_03_cat_count',\n 'ps_car_04_cat_count',\n 'ps_car_05_cat_count',\n 'ps_car_06_cat_count',\n 'ps_car_07_cat_count',\n 'ps_car_08_cat_count',\n 'ps_car_09_cat_count',\n 'ps_car_10_cat_count',\n 'ps_car_11_cat_count',\n 'mix_ind_count']"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_count_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 필요 없는 피처들\n",
    "drop_features = ['ps_ind_14' , 'ps_ind_10_bin' , 'ps_ind_11_bin' , 'ps_ind_12_bin' , 'ps_ind_13_bin' , 'ps_car_14']\n",
    "\n",
    "# remaining_features , cat_count_features 에서 drop_features를 제거한 데이터\n",
    "\n",
    "all_data_remaining = all_data[remaining_features + cat_count_features].drop(drop_features , axis = 1)\n",
    "\n",
    "# 데이터 합치기\n",
    "\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining) , encoded_cat_matrix] ,format = 'csr')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터 나누기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train :]\n",
    "\n",
    "y = train['target'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 하이퍼파라미터 최적화"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리(베이지안 최적화 수행용)\n",
    "\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X,y, test_size=0.2 , random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "\n",
    "bayes_dtrain = lgb.Dataset(X_train , y_train)\n",
    "bayes_dvalid = lgb.Dataset(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'num_leaves' : (30 , 40) ,\n",
    "                'lambda_l1' : (0.7 , 0.9),\n",
    "                'lambda_l2' : (0.9 , 1),\n",
    "                'feature_fraction' : (0.6 , 0.7),\n",
    "                'bagging_fraction' : (0.6 , 0.9),\n",
    "                'min_child_samples' : (6 , 10) ,\n",
    "                'min_child_weight' : (10 , 40)}\n",
    "\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "\n",
    "fixed_params = {'objective' : 'binary' ,\n",
    "                'learning_rate' : 0.005,\n",
    "                'bagging_freq' : 1,\n",
    "                'force_row_wise' : True,\n",
    "                'random_state' : 1991}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 베이지안 최적화용 평가지표 계산 함수 작성"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def eval_function(num_leaves , lambda_l1 , lambda_l2 , feature_fraction , bagging_fraction , min_child_samples , min_child_weight) :\n",
    "\n",
    "# 최적화하려는 평가지표(지니계수) 계산 함수\n",
    "\n",
    "# 베이지안 최적화를 수행할 하이퍼파라미터\n",
    "\n",
    "    params = {'num_leaves' : int(round(num_leaves)) ,\n",
    "              'lambda_l1' : lambda_l1,\n",
    "              'lambda_l2' : lambda_l2 ,\n",
    "              'feature_fraction' : feature_fraction ,\n",
    "              'bagging_fraction' : bagging_fraction,\n",
    "              'min_child_samples' : int(round(min_child_samples)) ,\n",
    "              'min_child_weight' : min_child_weight,\n",
    "              'feature_pre_filter' : False}\n",
    "\n",
    "    #하이퍼파라미터도 추가\n",
    "    params.update(fixed_params)\n",
    "\n",
    "    print('하이퍼파라미터 : ' , params)\n",
    "\n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params = params ,\n",
    "                          train_set = bayes_dtrain,\n",
    "                          num_boost_round= 2500,\n",
    "                          valid_sets= bayes_dvalid,\n",
    "                          feval = gini,\n",
    "                          early_stopping_rounds= 300,\n",
    "                          verbose_eval= False)\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = lgb_model.predict(X_valid)\n",
    "\n",
    "    # 지니계수 계산\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "\n",
    "    return gini_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 최적화 수행"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colorama==0.4.4 in c:\\users\\andyp\\anaconda3\\lib\\site-packages (0.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org colorama==0.4.4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization==1.4.0\n",
      "  Using cached bayesian_optimization-1.4.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\andyp\\anaconda3\\lib\\site-packages (from bayesian-optimization==1.4.0) (0.24.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\andyp\\anaconda3\\lib\\site-packages (from bayesian-optimization==1.4.0) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\andyp\\anaconda3\\lib\\site-packages (from bayesian-optimization==1.4.0) (1.20.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\andyp\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization==1.4.0) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\andyp\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization==1.4.0) (2.1.0)\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\andyp\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org bayesian-optimization==1.4.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "optimizer = BayesianOptimization(f = eval_function, # 평가지표 계산 함수\n",
    "                                pbounds = param_bounds, # 하이퍼파라미터 범위\n",
    "                                random_state = 0 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터 :  {'num_leaves': 34, 'lambda_l1': 0.8205526752143287, 'lambda_l2': 0.9544883182996897, 'feature_fraction': 0.6715189366372419, 'bagging_fraction': 0.7646440511781974, 'min_child_samples': 8, 'min_child_weight': 29.376823391999682, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2855811556220905\n",
      "\n",
      "| \u001B[0m1        \u001B[0m | \u001B[0m0.2856   \u001B[0m | \u001B[0m0.7646   \u001B[0m | \u001B[0m0.6715   \u001B[0m | \u001B[0m0.8206   \u001B[0m | \u001B[0m0.9545   \u001B[0m | \u001B[0m7.695    \u001B[0m | \u001B[0m29.38    \u001B[0m | \u001B[0m34.38    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 39, 'lambda_l1': 0.7766883037651555, 'lambda_l2': 0.9791725038082665, 'feature_fraction': 0.6963662760501029, 'bagging_fraction': 0.867531900234624, 'min_child_samples': 8, 'min_child_weight': 27.04133683281797, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2837380537005777\n",
      "\n",
      "| \u001B[0m2        \u001B[0m | \u001B[0m0.2837   \u001B[0m | \u001B[0m0.8675   \u001B[0m | \u001B[0m0.6964   \u001B[0m | \u001B[0m0.7767   \u001B[0m | \u001B[0m0.9792   \u001B[0m | \u001B[0m8.116    \u001B[0m | \u001B[0m27.04    \u001B[0m | \u001B[0m39.26    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 40, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'feature_fraction': 0.608712929970154, 'bagging_fraction': 0.6213108174593661, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2857848354322048\n",
      "\n",
      "| \u001B[95m3        \u001B[0m | \u001B[95m0.2858   \u001B[0m | \u001B[95m0.6213   \u001B[0m | \u001B[95m0.6087   \u001B[0m | \u001B[95m0.704    \u001B[0m | \u001B[95m0.9833   \u001B[0m | \u001B[95m9.113    \u001B[0m | \u001B[95m36.1     \u001B[0m | \u001B[95m39.79    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 30, 'lambda_l1': 0.8444997594874222, 'lambda_l2': 0.9234023852202012, 'feature_fraction': 0.6593983245038058, 'bagging_fraction': 0.8977977822397395, 'min_child_samples': 9, 'min_child_weight': 10.549362495448534, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2828993761731121\n",
      "\n",
      "| \u001B[0m4        \u001B[0m | \u001B[0m0.2829   \u001B[0m | \u001B[0m0.8978   \u001B[0m | \u001B[0m0.6594   \u001B[0m | \u001B[0m0.8445   \u001B[0m | \u001B[0m0.9234   \u001B[0m | \u001B[0m8.619    \u001B[0m | \u001B[0m10.55    \u001B[0m | \u001B[0m30.09    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 37, 'lambda_l1': 0.7738449330497988, 'lambda_l2': 0.9032695189818599, 'feature_fraction': 0.6606341064409726, 'bagging_fraction': 0.7666713964943057, 'min_child_samples': 9, 'min_child_weight': 29.306172421380474, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28513273331754563\n",
      "\n",
      "| \u001B[0m5        \u001B[0m | \u001B[0m0.2851   \u001B[0m | \u001B[0m0.7667   \u001B[0m | \u001B[0m0.6606   \u001B[0m | \u001B[0m0.7738   \u001B[0m | \u001B[0m0.9033   \u001B[0m | \u001B[0m8.769    \u001B[0m | \u001B[0m29.31    \u001B[0m | \u001B[0m36.6     \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 33, 'lambda_l1': 0.8178523882153511, 'lambda_l2': 0.9, 'feature_fraction': 0.6, 'bagging_fraction': 0.6, 'min_child_samples': 10, 'min_child_weight': 35.79651643178398, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2853891658385996\n",
      "\n",
      "| \u001B[0m6        \u001B[0m | \u001B[0m0.2854   \u001B[0m | \u001B[0m0.6      \u001B[0m | \u001B[0m0.6      \u001B[0m | \u001B[0m0.8179   \u001B[0m | \u001B[0m0.9      \u001B[0m | \u001B[0m9.967    \u001B[0m | \u001B[0m35.8     \u001B[0m | \u001B[0m32.59    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 37, 'lambda_l1': 0.8433793375135147, 'lambda_l2': 0.9479651949974717, 'feature_fraction': 0.6859622896374784, 'bagging_fraction': 0.8362539818721497, 'min_child_samples': 6, 'min_child_weight': 39.77484183530247, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2854766974907317\n",
      "\n",
      "| \u001B[0m7        \u001B[0m | \u001B[0m0.2855   \u001B[0m | \u001B[0m0.8363   \u001B[0m | \u001B[0m0.686    \u001B[0m | \u001B[0m0.8434   \u001B[0m | \u001B[0m0.948    \u001B[0m | \u001B[0m6.002    \u001B[0m | \u001B[0m39.77    \u001B[0m | \u001B[0m36.8     \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 30, 'lambda_l1': 0.7759269600824816, 'lambda_l2': 0.9, 'feature_fraction': 0.7, 'bagging_fraction': 0.6, 'min_child_samples': 10, 'min_child_weight': 28.21283616384366, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2847964155782387\n",
      "\n",
      "| \u001B[0m8        \u001B[0m | \u001B[0m0.2848   \u001B[0m | \u001B[0m0.6      \u001B[0m | \u001B[0m0.7      \u001B[0m | \u001B[0m0.7759   \u001B[0m | \u001B[0m0.9      \u001B[0m | \u001B[0m10.0     \u001B[0m | \u001B[0m28.21    \u001B[0m | \u001B[0m30.0     \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 36, 'lambda_l1': 0.7, 'lambda_l2': 1.0, 'feature_fraction': 0.7, 'bagging_fraction': 0.9, 'min_child_samples': 6, 'min_child_weight': 33.98699093132045, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.283740750776938\n",
      "\n",
      "| \u001B[0m9        \u001B[0m | \u001B[0m0.2837   \u001B[0m | \u001B[0m0.9      \u001B[0m | \u001B[0m0.7      \u001B[0m | \u001B[0m0.7      \u001B[0m | \u001B[0m1.0      \u001B[0m | \u001B[0m6.0      \u001B[0m | \u001B[0m33.99    \u001B[0m | \u001B[0m35.84    \u001B[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 베이지안 최적화 수행\n",
    "\n",
    "optimizer.maximize(init_points=  3 , n_iter = 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 결과확인"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "{'bagging_fraction': 0.6213108174593661,\n 'feature_fraction': 0.608712929970154,\n 'lambda_l1': 0.7040436794880651,\n 'lambda_l2': 0.9832619845547939,\n 'min_child_samples': 9.112627003799401,\n 'min_child_weight': 36.10036444740457,\n 'num_leaves': 39.78618342232764}"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가함수 점수가 최대일 대 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# 정수형 하이퍼파라미터 변환\n",
    "\n",
    "max_params['num_leaves'] = int(round(max_params['num_leaves']))\n",
    "max_params['min_child_samples'] = int(round(max_params['min_child_samples']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# 값이 고정된 하이퍼파라미터 추가\n",
    "\n",
    "max_params.update(fixed_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "{'bagging_fraction': 0.6213108174593661,\n 'feature_fraction': 0.608712929970154,\n 'lambda_l1': 0.7040436794880651,\n 'lambda_l2': 0.9832619845547939,\n 'min_child_samples': 9,\n 'min_child_weight': 36.10036444740457,\n 'num_leaves': 40,\n 'objective': 'binary',\n 'learning_rate': 0.005,\n 'bagging_freq': 1,\n 'force_row_wise': True,\n 'random_state': 1991}"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최종 하이퍼파라미터 출력\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 훈련 및 성능 검증"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\andyp\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153929\tvalid_0's gini: 0.297826\n",
      "[200]\tvalid_0's binary_logloss: 0.152575\tvalid_0's gini: 0.307987\n",
      "[300]\tvalid_0's binary_logloss: 0.151732\tvalid_0's gini: 0.315646\n",
      "[400]\tvalid_0's binary_logloss: 0.15115\tvalid_0's gini: 0.322832\n",
      "[500]\tvalid_0's binary_logloss: 0.150688\tvalid_0's gini: 0.330498\n",
      "[600]\tvalid_0's binary_logloss: 0.150302\tvalid_0's gini: 0.337661\n",
      "[700]\tvalid_0's binary_logloss: 0.149967\tvalid_0's gini: 0.344703\n",
      "[800]\tvalid_0's binary_logloss: 0.149665\tvalid_0's gini: 0.351174\n",
      "[900]\tvalid_0's binary_logloss: 0.149386\tvalid_0's gini: 0.357542\n",
      "[1000]\tvalid_0's binary_logloss: 0.14914\tvalid_0's gini: 0.363011\n",
      "[1100]\tvalid_0's binary_logloss: 0.1489\tvalid_0's gini: 0.368648\n",
      "[1200]\tvalid_0's binary_logloss: 0.148671\tvalid_0's gini: 0.373879\n",
      "[1300]\tvalid_0's binary_logloss: 0.148457\tvalid_0's gini: 0.378876\n",
      "[1400]\tvalid_0's binary_logloss: 0.148254\tvalid_0's gini: 0.383529\n",
      "[1500]\tvalid_0's binary_logloss: 0.148053\tvalid_0's gini: 0.388412\n",
      "[1600]\tvalid_0's binary_logloss: 0.147855\tvalid_0's gini: 0.393132\n",
      "[1700]\tvalid_0's binary_logloss: 0.147671\tvalid_0's gini: 0.397335\n",
      "[1800]\tvalid_0's binary_logloss: 0.147479\tvalid_0's gini: 0.401812\n",
      "[1900]\tvalid_0's binary_logloss: 0.147298\tvalid_0's gini: 0.406149\n",
      "[2000]\tvalid_0's binary_logloss: 0.147113\tvalid_0's gini: 0.410341\n",
      "[2100]\tvalid_0's binary_logloss: 0.146935\tvalid_0's gini: 0.414366\n",
      "[2200]\tvalid_0's binary_logloss: 0.146751\tvalid_0's gini: 0.418657\n",
      "[2300]\tvalid_0's binary_logloss: 0.146582\tvalid_0's gini: 0.422601\n",
      "[2400]\tvalid_0's binary_logloss: 0.146412\tvalid_0's gini: 0.426673\n",
      "[2500]\tvalid_0's binary_logloss: 0.146249\tvalid_0's gini: 0.430452\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's binary_logloss: 0.146249\tvalid_0's gini: 0.430452\n",
      "폴드 1  지니계수 : 0.43045153631463173\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154044\tvalid_0's gini: 0.285099\n",
      "[200]\tvalid_0's binary_logloss: 0.152769\tvalid_0's gini: 0.294886\n",
      "[300]\tvalid_0's binary_logloss: 0.151986\tvalid_0's gini: 0.302327\n",
      "[400]\tvalid_0's binary_logloss: 0.151437\tvalid_0's gini: 0.310055\n",
      "[500]\tvalid_0's binary_logloss: 0.151012\tvalid_0's gini: 0.317354\n",
      "[600]\tvalid_0's binary_logloss: 0.150656\tvalid_0's gini: 0.324008\n",
      "[700]\tvalid_0's binary_logloss: 0.150342\tvalid_0's gini: 0.330635\n",
      "[800]\tvalid_0's binary_logloss: 0.150055\tvalid_0's gini: 0.336887\n",
      "[900]\tvalid_0's binary_logloss: 0.149793\tvalid_0's gini: 0.34274\n",
      "[1000]\tvalid_0's binary_logloss: 0.149536\tvalid_0's gini: 0.348577\n",
      "[1100]\tvalid_0's binary_logloss: 0.149294\tvalid_0's gini: 0.354284\n",
      "[1200]\tvalid_0's binary_logloss: 0.149065\tvalid_0's gini: 0.359531\n",
      "[1300]\tvalid_0's binary_logloss: 0.148847\tvalid_0's gini: 0.36461\n",
      "[1400]\tvalid_0's binary_logloss: 0.148643\tvalid_0's gini: 0.369238\n",
      "[1500]\tvalid_0's binary_logloss: 0.148446\tvalid_0's gini: 0.373856\n",
      "[1600]\tvalid_0's binary_logloss: 0.148244\tvalid_0's gini: 0.378513\n",
      "[1700]\tvalid_0's binary_logloss: 0.148049\tvalid_0's gini: 0.383028\n",
      "[1800]\tvalid_0's binary_logloss: 0.14786\tvalid_0's gini: 0.387413\n",
      "[1900]\tvalid_0's binary_logloss: 0.14767\tvalid_0's gini: 0.391896\n",
      "[2000]\tvalid_0's binary_logloss: 0.147487\tvalid_0's gini: 0.396185\n",
      "[2100]\tvalid_0's binary_logloss: 0.147311\tvalid_0's gini: 0.400129\n",
      "[2200]\tvalid_0's binary_logloss: 0.147136\tvalid_0's gini: 0.40407\n",
      "[2300]\tvalid_0's binary_logloss: 0.146961\tvalid_0's gini: 0.40802\n",
      "[2400]\tvalid_0's binary_logloss: 0.14679\tvalid_0's gini: 0.412142\n",
      "[2500]\tvalid_0's binary_logloss: 0.146622\tvalid_0's gini: 0.416\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's binary_logloss: 0.146622\tvalid_0's gini: 0.416\n",
      "폴드 2  지니계수 : 0.41599986558989005\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153916\tvalid_0's gini: 0.291575\n",
      "[200]\tvalid_0's binary_logloss: 0.152573\tvalid_0's gini: 0.300826\n",
      "[300]\tvalid_0's binary_logloss: 0.151731\tvalid_0's gini: 0.308526\n",
      "[400]\tvalid_0's binary_logloss: 0.151146\tvalid_0's gini: 0.316146\n",
      "[500]\tvalid_0's binary_logloss: 0.150697\tvalid_0's gini: 0.323212\n",
      "[600]\tvalid_0's binary_logloss: 0.150325\tvalid_0's gini: 0.329874\n",
      "[700]\tvalid_0's binary_logloss: 0.150007\tvalid_0's gini: 0.336197\n",
      "[800]\tvalid_0's binary_logloss: 0.149721\tvalid_0's gini: 0.342142\n",
      "[900]\tvalid_0's binary_logloss: 0.14946\tvalid_0's gini: 0.347916\n",
      "[1000]\tvalid_0's binary_logloss: 0.149224\tvalid_0's gini: 0.353072\n",
      "[1100]\tvalid_0's binary_logloss: 0.149002\tvalid_0's gini: 0.358182\n",
      "[1200]\tvalid_0's binary_logloss: 0.14878\tvalid_0's gini: 0.363267\n",
      "[1300]\tvalid_0's binary_logloss: 0.148571\tvalid_0's gini: 0.368104\n",
      "[1400]\tvalid_0's binary_logloss: 0.148376\tvalid_0's gini: 0.372453\n",
      "[1500]\tvalid_0's binary_logloss: 0.148179\tvalid_0's gini: 0.37712\n",
      "[1600]\tvalid_0's binary_logloss: 0.147985\tvalid_0's gini: 0.38163\n",
      "[1700]\tvalid_0's binary_logloss: 0.147796\tvalid_0's gini: 0.386097\n",
      "[1800]\tvalid_0's binary_logloss: 0.147605\tvalid_0's gini: 0.3905\n",
      "[1900]\tvalid_0's binary_logloss: 0.14743\tvalid_0's gini: 0.394776\n",
      "[2000]\tvalid_0's binary_logloss: 0.147257\tvalid_0's gini: 0.398703\n",
      "[2100]\tvalid_0's binary_logloss: 0.147083\tvalid_0's gini: 0.402504\n",
      "[2200]\tvalid_0's binary_logloss: 0.14691\tvalid_0's gini: 0.406516\n",
      "[2300]\tvalid_0's binary_logloss: 0.146729\tvalid_0's gini: 0.410652\n",
      "[2400]\tvalid_0's binary_logloss: 0.14656\tvalid_0's gini: 0.414681\n",
      "[2500]\tvalid_0's binary_logloss: 0.14639\tvalid_0's gini: 0.418588\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's binary_logloss: 0.14639\tvalid_0's gini: 0.418588\n",
      "폴드 3  지니계수 : 0.4185875528897569\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154037\tvalid_0's gini: 0.283031\n",
      "[200]\tvalid_0's binary_logloss: 0.152752\tvalid_0's gini: 0.292404\n",
      "[300]\tvalid_0's binary_logloss: 0.151959\tvalid_0's gini: 0.299947\n",
      "[400]\tvalid_0's binary_logloss: 0.151401\tvalid_0's gini: 0.307543\n",
      "[500]\tvalid_0's binary_logloss: 0.150978\tvalid_0's gini: 0.31462\n",
      "[600]\tvalid_0's binary_logloss: 0.150627\tvalid_0's gini: 0.321184\n",
      "[700]\tvalid_0's binary_logloss: 0.150317\tvalid_0's gini: 0.327634\n",
      "[800]\tvalid_0's binary_logloss: 0.150029\tvalid_0's gini: 0.334011\n",
      "[900]\tvalid_0's binary_logloss: 0.149769\tvalid_0's gini: 0.339993\n",
      "[1000]\tvalid_0's binary_logloss: 0.149521\tvalid_0's gini: 0.345545\n",
      "[1100]\tvalid_0's binary_logloss: 0.149293\tvalid_0's gini: 0.351003\n",
      "[1200]\tvalid_0's binary_logloss: 0.149073\tvalid_0's gini: 0.356248\n",
      "[1300]\tvalid_0's binary_logloss: 0.148871\tvalid_0's gini: 0.361182\n",
      "[1400]\tvalid_0's binary_logloss: 0.148677\tvalid_0's gini: 0.365689\n",
      "[1500]\tvalid_0's binary_logloss: 0.148476\tvalid_0's gini: 0.370684\n",
      "[1600]\tvalid_0's binary_logloss: 0.148287\tvalid_0's gini: 0.375168\n",
      "[1700]\tvalid_0's binary_logloss: 0.148096\tvalid_0's gini: 0.379801\n",
      "[1800]\tvalid_0's binary_logloss: 0.147914\tvalid_0's gini: 0.384194\n",
      "[1900]\tvalid_0's binary_logloss: 0.14773\tvalid_0's gini: 0.388672\n",
      "[2000]\tvalid_0's binary_logloss: 0.147553\tvalid_0's gini: 0.392803\n",
      "[2100]\tvalid_0's binary_logloss: 0.147369\tvalid_0's gini: 0.397084\n",
      "[2200]\tvalid_0's binary_logloss: 0.147192\tvalid_0's gini: 0.40121\n",
      "[2300]\tvalid_0's binary_logloss: 0.147016\tvalid_0's gini: 0.405269\n",
      "[2400]\tvalid_0's binary_logloss: 0.146852\tvalid_0's gini: 0.409275\n",
      "[2500]\tvalid_0's binary_logloss: 0.146681\tvalid_0's gini: 0.413347\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's binary_logloss: 0.146681\tvalid_0's gini: 0.413347\n",
      "폴드 4  지니계수 : 0.4133467618166041\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.15439\tvalid_0's gini: 0.26681\n",
      "[200]\tvalid_0's binary_logloss: 0.15338\tvalid_0's gini: 0.272186\n",
      "[300]\tvalid_0's binary_logloss: 0.152821\tvalid_0's gini: 0.275897\n",
      "[400]\tvalid_0's binary_logloss: 0.1525\tvalid_0's gini: 0.278734\n",
      "[500]\tvalid_0's binary_logloss: 0.152277\tvalid_0's gini: 0.282151\n",
      "[600]\tvalid_0's binary_logloss: 0.15212\tvalid_0's gini: 0.285039\n",
      "[700]\tvalid_0's binary_logloss: 0.152009\tvalid_0's gini: 0.287435\n",
      "[800]\tvalid_0's binary_logloss: 0.15192\tvalid_0's gini: 0.289549\n",
      "[900]\tvalid_0's binary_logloss: 0.151862\tvalid_0's gini: 0.290886\n",
      "[1000]\tvalid_0's binary_logloss: 0.151819\tvalid_0's gini: 0.291935\n",
      "[1100]\tvalid_0's binary_logloss: 0.151782\tvalid_0's gini: 0.292972\n",
      "[1200]\tvalid_0's binary_logloss: 0.151752\tvalid_0's gini: 0.293784\n",
      "[1300]\tvalid_0's binary_logloss: 0.151732\tvalid_0's gini: 0.294315\n",
      "[1400]\tvalid_0's binary_logloss: 0.151724\tvalid_0's gini: 0.294475\n",
      "[1500]\tvalid_0's binary_logloss: 0.151713\tvalid_0's gini: 0.294786\n",
      "[1600]\tvalid_0's binary_logloss: 0.1517\tvalid_0's gini: 0.295146\n",
      "[1700]\tvalid_0's binary_logloss: 0.151694\tvalid_0's gini: 0.295268\n",
      "[1800]\tvalid_0's binary_logloss: 0.151695\tvalid_0's gini: 0.295212\n",
      "[1900]\tvalid_0's binary_logloss: 0.151689\tvalid_0's gini: 0.295454\n",
      "[2000]\tvalid_0's binary_logloss: 0.151693\tvalid_0's gini: 0.2954\n",
      "[2100]\tvalid_0's binary_logloss: 0.151694\tvalid_0's gini: 0.295427\n",
      "[2200]\tvalid_0's binary_logloss: 0.151692\tvalid_0's gini: 0.295538\n",
      "[2300]\tvalid_0's binary_logloss: 0.151699\tvalid_0's gini: 0.295411\n",
      "Early stopping, best iteration is:\n",
      "[2045]\tvalid_0's binary_logloss: 0.151689\tvalid_0's gini: 0.295553\n",
      "폴드 5  지니계수 : 0.29555250456072807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기 생성\n",
    "folds = StratifiedKFold(n_splits=5 , shuffle = True , random_state= 1991)\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# OOF 방식으로 모델 훈련 ,검증 , 예측\n",
    "\n",
    "for idx, (train , valid_idx) in enumerate(folds.split(X,y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40 , f'폴드 {idx+1} / 폴드 {folds.n_splits}' , '#'*40)\n",
    "\n",
    "    X_train , y_train = X[train_idx] , y[train_idx] # 훈련용 데이터\n",
    "    X_valid , y_valid = X[valid_idx] , y[valid_idx] # 검증용 데이터\n",
    "\n",
    "    # LightGBM 전용 데이터셋 생성\n",
    "    dtrain = lgb.Dataset(X_train , y_train) # LightGBM 전용 훈련 데이터셋\n",
    "    dvalid = lgb.Dataset(X_valid , y_valid) # LightGBM 전용 검증 데이터셋\n",
    "\n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params = max_params , # 최적 하이퍼파라미터\n",
    "                          train_set = dtrain, # 훈련 데이터 셋\n",
    "                          num_boost_round= 2500, # 부스팅 반복 횟수\n",
    "                          valid_sets= dvalid , # 성능 평가용 검증 데이터셋\n",
    "                          feval = gini, # 검증용 평가지표\n",
    "                          early_stopping_rounds= 300, # 조기종료 조건\n",
    "                          verbose_eval = 100) # 100 번째 마다 점수 출력\n",
    "\n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "    oof_test_preds += lgb_model.predict(X_test) / folds.n_splits\n",
    "\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "    oof_test_preds_lgb = oof_test_preds\n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1}  지니계수 : {gini_score}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "(595212,)"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "(595212,)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_val_preds.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF 검증 데이터 지니계수 : 0.3949215102693418\n"
     ]
    }
   ],
   "source": [
    "print('OOF 검증 데이터 지니계수 :' , eval_gini(y, oof_val_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost 피처 엔지니어링"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# LightGBM 용 gini() 함수\n",
    "def gini(preds , dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini' , eval_gini(labels , preds) , True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# XGBoost용 gini() 함수\n",
    "\n",
    "def gini(preds , dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini' , eval_gini(labels,preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 하이퍼파라미터 최적화"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 데이터 셋 준비"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8:2 비율로 훈련 데이터 , 검증 데이터 분리( 베이지안 최적화 수행용)\n",
    "\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X,y, test_size=0.2 , random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "\n",
    "bayes_dtrain = xgb.DMatrix(X_train , y_train)\n",
    "bayes_dvalid = xgb.DMatrix(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 하이퍼파라미터 범위 설정"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'max_depth' : (4 , 8) ,\n",
    "                'subsample' : (0.6 , 0.9),\n",
    "                'colsample_bytree' : (0.7 , 1.0),\n",
    "                'min_child_weight' : (5 , 7),\n",
    "                'gamma' : (8 , 11),\n",
    "                'reg_alpha' : (7 , 9) ,\n",
    "                'reg_lambda' : (1.1 , 1.5),\n",
    "                'scale_pos_weight' : (1.4 , 1.6)}\n",
    "\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "\n",
    "fixed_params = {'objective' : 'binary:logistic' ,\n",
    "                'learning_rate' : 0.02,\n",
    "                'random_state' : 1991}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def eval_function(max_depth , subsample , colsample_bytree , min_child_weight , reg_alpha , gamma , reg_lambda , scale_pos_weight) :\n",
    "\n",
    "    # 최적화하려는 평가지표(지니계수) 계산 함수\n",
    "\n",
    "    # 베이지안 최적화를 수행할 하이퍼파라미터\n",
    "\n",
    "    params = {'max_depth' : int(round(max_depth)) ,\n",
    "              'subsample' : subsample,\n",
    "              'colsample_bytree' : colsample_bytree ,\n",
    "              'min_child_weight' : min_child_weight,\n",
    "              'gamma' : gamma,\n",
    "              'reg_alpha' : reg_alpha,\n",
    "              'reg_lambda' : reg_lambda,\n",
    "              'scale_pos_weight' : scale_pos_weight}\n",
    "\n",
    "    # 값이 고정된 하이퍼파라미터도 추가\n",
    "    params.update(fixed_params)\n",
    "\n",
    "    print('하이퍼파라미터 : ' , params)\n",
    "\n",
    "    # XGBoost 모델 훈련\n",
    "    xgb_model = xgb.train(params = params ,\n",
    "                          dtrain = bayes_dtrain,\n",
    "                          num_boost_round= 2000,\n",
    "                          evals = [(bayes_dvalid , ' bayes_dvalid')],\n",
    "                          maximize = True,\n",
    "                          feval = gini,\n",
    "                          early_stopping_rounds= 200,\n",
    "                          verbose_eval= False)\n",
    "\n",
    "    best_iter = xgb_model.best_iteration # 최적 반복횟수\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = xgb_model.predict(bayes_dvalid , iteration_range=(0, best_iter))\n",
    "\n",
    "    # 지니계수 계산\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "\n",
    "    return gini_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 최적화 수행"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |   gamma   | max_depth | min_ch... | reg_alpha | reg_la... | scale_... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터 :  {'max_depth': 6, 'subsample': 0.867531900234624, 'colsample_bytree': 0.8646440511781974, 'min_child_weight': 6.0897663659937935, 'gamma': 10.14556809911726, 'reg_alpha': 7.84730959867781, 'reg_lambda': 1.3583576452266626, 'scale_pos_weight': 1.4875174422525386, 'objective': 'binary:logistic', 'learning_rate': 0.02, 'random_state': 1991}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "\n",
    "optimizer = BayesianOptimization(f= eval_function, pbounds = param_bounds , random_state= 0)\n",
    "\n",
    "\n",
    "# 베이지안 최적화 수행\n",
    "\n",
    "optimizer.maximize(init_points= 3 , n_iter= 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 결과확인"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 평가함수 점수가 최대일 때 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 정수형 하이퍼파라미터 변환\n",
    "\n",
    "max_params['max_depth'] = int(round(max_params['max_depth']))\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터 추가\n",
    "\n",
    "max_params.update(fixed_params)\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 훈련 및 성능 검증"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기 생성\n",
    "folds = StratifiedKFold(n_splits= 5 , shuffle= True , random_state= 1991)\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "\n",
    "# OOF 방식으로 훈련된 모델 훈련 , 검증 , 예측\n",
    "\n",
    "for idx , (train_idx , valid_idx) in enumerate(folds.split(X,y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "\n",
    "    print('#' *40,  f'폴드 {idx+1} / 폴드 {folds.n_splits}' , '#'*40)\n",
    "\n",
    "\n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train , y_train = X[train_idx] , y[train_idx]\n",
    "    X_valid , y_valid = X[valid_idx] , y[valid_idx]\n",
    "\n",
    "    #XGBoost 전용 데이터셋 생성\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train , y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid , y_valid)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    #XGBoost 모델 훈련\n",
    "    xgb_model = xgb.train(params = max_params,\n",
    "                          dtrain = dtrain,\n",
    "                          num_boost_round = 2000,\n",
    "                          evals = [(dvalid , 'valid')],\n",
    "                          maximize = True,\n",
    "                          feval = gini,\n",
    "                          early_stopping_rounds = 200,\n",
    "                          verbose_eval = 100)\n",
    "\n",
    "    # 모델 성능이 가장 좋을 때의 부스팅 반복 횟수 저장\n",
    "    best_iter= xgb_model.best_iteration\n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "\n",
    "    oof_test_preds += xgb_model.predict(dtest, iteration_range = (0 , best_iter))/ folds.n_splits\n",
    "\n",
    "    oof_test_preds_xgb = oof_test_preds\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "    oof_val_preds[valid_idx] += xgb_model.predict(dvalid , iteration_range=(0, best_iter))\n",
    "\n",
    "\n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "    gini_score = eval_gini(y_valid , oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1}  지니계수 : {gini_score}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('OOF 검증 데이터 지니계수 : ' , eval_gini(y , oof_val_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 성능 개선 3 : LightGBM 과 XGBoost 앙상블"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "oof_test_preds = oof_test_preds_lgb *0.5 + oof_test_preds_xgb * 0.5\n",
    "\n",
    "\n",
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}