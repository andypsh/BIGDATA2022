{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 경로\n",
    "\n",
    "data_path = './porto-seguro-safe-driver-prediction/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv' , index_col = 'id')\n",
    "test = pd.read_csv(data_path + 'test.csv' , index_col = 'id')\n",
    "submission = pd.read_csv(data_path + 'sample_submission.csv' , index_col ='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "         ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n0                2              2          5              1              0   \n1                1              1          7              0              0   \n2                5              4          9              1              0   \n3                0              1          2              0              0   \n4                0              2          0              1              0   \n...            ...            ...        ...            ...            ...   \n1488023          0              1          6              0              0   \n1488024          5              3          5              1              0   \n1488025          0              1          5              0              0   \n1488026          6              1          5              1              0   \n1488027          7              1          4              1              0   \n\n         ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  \\\n0                    0              1              0              0   \n1                    0              0              1              0   \n2                    0              0              1              0   \n3                    1              0              0              0   \n4                    1              0              0              0   \n...                ...            ...            ...            ...   \n1488023              0              1              0              0   \n1488024              0              0              1              0   \n1488025              1              0              0              0   \n1488026              0              0              0              1   \n1488027              0              0              0              1   \n\n         ps_ind_10_bin  ...  ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  \\\n0                    0  ...           9           1           5           8   \n1                    0  ...           3           1           1           9   \n2                    0  ...           4           2           7           7   \n3                    0  ...           2           2           4           9   \n4                    0  ...           3           1           1           3   \n...                ...  ...         ...         ...         ...         ...   \n1488023              0  ...           4           2           3           4   \n1488024              0  ...           6           2           2          11   \n1488025              0  ...           5           2           2          11   \n1488026              0  ...           1           1           2           7   \n1488027              0  ...           5           2           2           7   \n\n         ps_calc_15_bin  ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  \\\n0                     0               1               1               0   \n1                     0               1               1               0   \n2                     0               1               1               0   \n3                     0               0               0               0   \n4                     0               0               0               1   \n...                 ...             ...             ...             ...   \n1488023               0               1               0               0   \n1488024               0               0               1               1   \n1488025               0               1               1               0   \n1488026               1               1               0               0   \n1488027               0               1               1               1   \n\n         ps_calc_19_bin  ps_calc_20_bin  \n0                     0               1  \n1                     1               0  \n2                     1               0  \n3                     0               0  \n4                     1               0  \n...                 ...             ...  \n1488023               1               0  \n1488024               0               0  \n1488025               0               0  \n1488026               0               0  \n1488027               0               0  \n\n[1488028 rows x 57 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ps_ind_01</th>\n      <th>ps_ind_02_cat</th>\n      <th>ps_ind_03</th>\n      <th>ps_ind_04_cat</th>\n      <th>ps_ind_05_cat</th>\n      <th>ps_ind_06_bin</th>\n      <th>ps_ind_07_bin</th>\n      <th>ps_ind_08_bin</th>\n      <th>ps_ind_09_bin</th>\n      <th>ps_ind_10_bin</th>\n      <th>...</th>\n      <th>ps_calc_11</th>\n      <th>ps_calc_12</th>\n      <th>ps_calc_13</th>\n      <th>ps_calc_14</th>\n      <th>ps_calc_15_bin</th>\n      <th>ps_calc_16_bin</th>\n      <th>ps_calc_17_bin</th>\n      <th>ps_calc_18_bin</th>\n      <th>ps_calc_19_bin</th>\n      <th>ps_calc_20_bin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>5</td>\n      <td>8</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>9</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>4</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>4</td>\n      <td>2</td>\n      <td>7</td>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1488023</th>\n      <td>0</td>\n      <td>1</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>4</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1488024</th>\n      <td>5</td>\n      <td>3</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>2</td>\n      <td>2</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1488025</th>\n      <td>0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2</td>\n      <td>11</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1488026</th>\n      <td>6</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>7</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1488027</th>\n      <td>7</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2</td>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1488028 rows × 57 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.concat([train , test] , ignore_index= True)\n",
    "all_data = all_data.drop('target' , axis = 1) # 타깃값 제거\n",
    "\n",
    "all_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n       'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n       'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n       'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n       'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n       'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n       'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n       'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n       'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n       'ps_calc_20_bin'],\n      dtype='object')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = all_data.columns # 전체 피처\n",
    "all_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 명목형 피처 원-핫 인코딩"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<1488028x184 sparse matrix of type '<class 'numpy.float64'>'\n\twith 20832392 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 명목형 피처 추출\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "# 이름에 cat이 포함된 피처가 명목형 피처이다.\n",
    "onehot_encoder = OneHotEncoder() # 원-핫 인코더 객체 생성\n",
    "\n",
    "# 인코딩\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])\n",
    "\n",
    "encoded_cat_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 추가로 제거할 피처\n",
    "drop_features = ['ps_ind14' , 'ps_ind_10_bin' , 'ps_ind_11_bin' , 'ps_ind_12_bin' , 'ps_ind_13_bin' , 'ps_car_14' ]\n",
    "\n",
    "# 1> 명목형 피처 , 2> calc 분류의 피처 , 3> 추가 제거할 피처를 제외한 피처\n",
    "\n",
    "remaining_features = [feature for feature in all_features\n",
    "                      if ('cat' not in feature and\n",
    "                          'calc' not in feature and\n",
    "                          feature not in drop_features)]\n",
    "# cat(명목형) 피처 , calc(피처) , 추가 제거할 6개피처를 제외한 나머지 피처를 remaining_features 에 저장"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<1488028x202 sparse matrix of type '<class 'numpy.float64'>'\n\twith 37644877 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data[remaining_features]) , encoded_cat_matrix] , format= 'csr')\n",
    "\n",
    "all_data_sprs\n",
    "\n",
    "# CSR  형식으로 바꾸어 hstack()으로 행렬을 수평 방향으로 합친다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train :]\n",
    "\n",
    "y = train['target'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 지니계수"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 지니계수란?\n",
    "\n",
    "# 소득 불평등 정도를 나타내는 지표. 지니계수가 작을수록 소득 수준이 평등하고, 클수록 불평등함을 의미한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 정규화 지니계수 계산 함수\n",
    "\n",
    "# 정규화란 값의 범위를 0~1 사이로 조정한다는 의미, 정규화 지니계수는 값이 0에 가까울수록 성능이 나쁘고, 1에 가까울 수록 성능이 좋다는 의미가 된다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_gini(y_true , y_pred):\n",
    "    # 실제값과 예측값의 크기가 서로 같은지 확인(값이 다르면 오류 발생)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    n_samples = y_true.shape[0] # 데이터 개수\n",
    "    L_mid = np.linspace(1/ n_samples ,1 , n_samples) # 대각선 값\n",
    "\n",
    "    # 1) 예측값에 대한 지니계수\n",
    "\n",
    "    pred_order = y_true[y_pred.argsort()] # y_pred 크기순으로 y_true 값 정렬\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order) # 로렌츠 곡선\n",
    "\n",
    "    G_pred = np.sum(L_mid - L_pred) # 예측값에 대한 지니계수\n",
    "\n",
    "    # 2) 예측이 완벽할 때 지니계쑤\n",
    "\n",
    "    true_order = y_true[y_true.argsort()] # y_true 크기순으로 y_true 값 정렬\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order) # 로렌츠 곡선\n",
    "    G_true = np.sum(L_mid - L_true) # 예측이 완벽할 때 지니계수\n",
    "\n",
    "    # 정규화된 지니계수\n",
    "    return G_pred / G_true"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# LightGBM 용 gini() 함수\n",
    "\n",
    "def gini(preds , dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "\n",
    "    return 'gini' , eval_gini(labels , preds ) , True\n",
    "\n",
    "# 'gini' : 평가지표이름 , eval_gini(labels,preds) : 평가점수 , True : 평가 점수가 높을수록 좋은지 여부"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 훈련 및 성능 검증"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# OOF(Out of Fold prediction) 예측 방식\n",
    "\n",
    "# K 폴드 교차 검증을 수행하면서 각 폴드마다 테스트 데이터로 예측하는 방식이다.\n",
    "\n",
    "# K 폴드 교차 검ㅈ응을 하면서 폴드마다 1> 훈련 데이터로 모델을 훈련하고, 2> 검증 데이터로 모델 성능을 측정하며 , 3> 테스트 데이터로 최종 타깃 확률도 예측한다. 훈련된 모델로 마지막에 한 번만 예측하는 것이 아니다. 각 폴드별 모델로 여러번 예측해 평균을 내는 방식이다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# OOF 방식으로 LightGBM 훈련\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기\n",
    "\n",
    "folds = StratifiedKFold(n_splits= 5 , shuffle= True , random_state= 1991)\n",
    "\n",
    "# 층화 K 폴드 교차 검증기는 타깃값이 불균형하므로 K폴드가 아닌 층화 K폴드를 수행하는 게 바람직하다. 층화 K폴드는 타깃값이 균등하게\n",
    "# 폴드를 나누는 방식이기 때문이다.\n",
    "\n",
    "\n",
    "# n_splits 파라미터로 전달한 수만큼 폴드를 나눈다. 여기서는 5개로 나누었다. shuffle = True 를 전달하면 폴드를 나눌때 데이터를 섞어준다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# LightGBM의 하이퍼파라미터를 설정한다. LightGBM은 하이퍼파라미터를 갖고 있지만, 여기서는 4가지만 설정한다.\n",
    "\n",
    "params = {'objective' : 'binary' , 'learning_rate' : 0.01 , 'force_row_wise' : True , 'random_state' : 0}\n",
    "\n",
    "# 이진분류 문제이므로 objective 파라미터는 binary로 설정했다. 학습률은 0.01로, 랜덤 스테이트 값은 9으로 설정했다.\n",
    "# force_row_wise : True 는 경고 문구를 없애려고 추가한 파라미터이다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_val_preds = np.zeros(X.shape[0])\n",
    "\n",
    "# ==> oof_val_preds 는 검증 데이터를 활용해 예측한 확률값을 저장하는 배열이다. K 폴드로 나누어도 훈련 데이터 전체가 결국엔 한 번씩 검증 데이터로 활용된다. 따라서 oof_val_preds 배열 크기는 훈련 데이터와 같아야 한다.\n",
    "# 훈련 데이터 개수는 X.shpae[0]으로 구한다.\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# oof_test_preds는 테스트 데이터를 활용해 예측한 확률값을 저장하는 배열이다. 최종 제출에 사용할 값이므로 크기는 테스트 데이터와 같아야한다. 테스트 데이터 개수는 X_test.shape[0]으로 구한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.3.5-py3-none-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 13.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from lightgbm) (0.24.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from lightgbm) (1.6.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from lightgbm) (1.20.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\user\\anaconda3\\lib\\site-packages (from lightgbm) (0.36.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.3.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org lightgbm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1100\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153356\tvalid_0's gini: 0.261373\n",
      "[200]\tvalid_0's binary_logloss: 0.152424\tvalid_0's gini: 0.27589\n",
      "[300]\tvalid_0's binary_logloss: 0.152031\tvalid_0's gini: 0.281949\n",
      "[400]\tvalid_0's binary_logloss: 0.151802\tvalid_0's gini: 0.286194\n",
      "[500]\tvalid_0's binary_logloss: 0.151737\tvalid_0's gini: 0.286881\n",
      "[600]\tvalid_0's binary_logloss: 0.151686\tvalid_0's gini: 0.287701\n",
      "[700]\tvalid_0's binary_logloss: 0.151677\tvalid_0's gini: 0.287901\n",
      "Early stopping, best iteration is:\n",
      "[655]\tvalid_0's binary_logloss: 0.151674\tvalid_0's gini: 0.287982\n",
      "폴드 1 지니계수 : 0.2879824293420261\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1098\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274764\n",
      "[LightGBM] [Info] Start training from score -3.274764\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153494\tvalid_0's gini: 0.249939\n",
      "[200]\tvalid_0's binary_logloss: 0.152711\tvalid_0's gini: 0.260705\n",
      "[300]\tvalid_0's binary_logloss: 0.152396\tvalid_0's gini: 0.267007\n",
      "[400]\tvalid_0's binary_logloss: 0.152249\tvalid_0's gini: 0.271043\n",
      "[500]\tvalid_0's binary_logloss: 0.152185\tvalid_0's gini: 0.272626\n",
      "[600]\tvalid_0's binary_logloss: 0.152165\tvalid_0's gini: 0.273257\n",
      "[700]\tvalid_0's binary_logloss: 0.152163\tvalid_0's gini: 0.27313\n",
      "Early stopping, best iteration is:\n",
      "[653]\tvalid_0's binary_logloss: 0.152158\tvalid_0's gini: 0.273406\n",
      "폴드 2 지니계수 : 0.2734064630206154\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17356, number of negative: 458814\n",
      "[LightGBM] [Info] Total Bins 1102\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036449 -> initscore=-3.274707\n",
      "[LightGBM] [Info] Start training from score -3.274707\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153263\tvalid_0's gini: 0.261144\n",
      "[200]\tvalid_0's binary_logloss: 0.152341\tvalid_0's gini: 0.271587\n",
      "[300]\tvalid_0's binary_logloss: 0.151976\tvalid_0's gini: 0.276629\n",
      "[400]\tvalid_0's binary_logloss: 0.151818\tvalid_0's gini: 0.278742\n",
      "[500]\tvalid_0's binary_logloss: 0.151759\tvalid_0's gini: 0.279933\n",
      "[600]\tvalid_0's binary_logloss: 0.151759\tvalid_0's gini: 0.280056\n",
      "Early stopping, best iteration is:\n",
      "[541]\tvalid_0's binary_logloss: 0.151753\tvalid_0's gini: 0.280206\n",
      "폴드 3 지니계수 : 0.2802057321746898\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1100\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153399\tvalid_0's gini: 0.25056\n",
      "[200]\tvalid_0's binary_logloss: 0.152558\tvalid_0's gini: 0.262678\n",
      "[300]\tvalid_0's binary_logloss: 0.152249\tvalid_0's gini: 0.267308\n",
      "[400]\tvalid_0's binary_logloss: 0.152117\tvalid_0's gini: 0.269741\n",
      "[500]\tvalid_0's binary_logloss: 0.152071\tvalid_0's gini: 0.270738\n",
      "[600]\tvalid_0's binary_logloss: 0.152061\tvalid_0's gini: 0.27121\n",
      "[700]\tvalid_0's binary_logloss: 0.152069\tvalid_0's gini: 0.27145\n",
      "Early stopping, best iteration is:\n",
      "[608]\tvalid_0's binary_logloss: 0.152061\tvalid_0's gini: 0.271241\n",
      "폴드 4 지니계수 : 0.2712407835640063\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1103\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 201\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153481\tvalid_0's gini: 0.261934\n",
      "[200]\tvalid_0's binary_logloss: 0.152645\tvalid_0's gini: 0.273354\n",
      "[300]\tvalid_0's binary_logloss: 0.152268\tvalid_0's gini: 0.280299\n",
      "[400]\tvalid_0's binary_logloss: 0.152071\tvalid_0's gini: 0.284938\n",
      "[500]\tvalid_0's binary_logloss: 0.152005\tvalid_0's gini: 0.286724\n",
      "[600]\tvalid_0's binary_logloss: 0.151986\tvalid_0's gini: 0.28717\n",
      "[700]\tvalid_0's binary_logloss: 0.151979\tvalid_0's gini: 0.287158\n",
      "Early stopping, best iteration is:\n",
      "[658]\tvalid_0's binary_logloss: 0.151973\tvalid_0's gini: 0.287407\n",
      "폴드 5 지니계수 : 0.2874066737367479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# OOF 방식으로 모델 훈련 , 검증 , 예측\n",
    "\n",
    "for idx, (train_idx , valid_idx) in enumerate(folds.split(X, y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40 , f'폴드 {idx +1} / 폴드 {folds.n_splits}' , '#'*40)\n",
    "\n",
    "    # 훈련용 데이터, 검증용 데이터 설정\n",
    "    X_train , y_train = X[train_idx] , y[train_idx] # 훈련용 데이터\n",
    "    X_valid , y_valid = X[valid_idx] , y[valid_idx] # 검증용 데이터\n",
    "\n",
    "    # LightGBM 전용 데이터셋 생성\n",
    "    dtrain = lgb.Dataset(X_train , y_train) # LightGBM 전용 훈련 데이터 셋\n",
    "    dvalid = lgb.Dataset(X_valid , y_valid) # LightGBM 전용 검증 데이터 셋\n",
    "\n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params = params , # 훈련용 하이퍼파라미터\n",
    "                          train_set = dtrain, # 훈련 데이터 셋\n",
    "                          num_boost_round = 1000, # 부스팅 반복 횟수\n",
    "                          valid_sets=  dvalid ,  # 성능 평가용 검증 데이터 셋\n",
    "                          feval = gini, # 검증용 평가지표\n",
    "                          early_stopping_rounds = 100, # 조기종료 조건\n",
    "                          verbose_eval = 100 ) # 100번째마다 점수 출력\n",
    "\n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "\n",
    "    oof_test_preds += lgb_model.predict(X_test)/folds.n_splits\n",
    "\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "\n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "\n",
    "    gini_score = eval_gini(y_valid , oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx +1} 지니계수 : {gini_score}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF 검증 데이터 지니계수 :  0.27992426994281666\n"
     ]
    }
   ],
   "source": [
    "print('OOF 검증 데이터 지니계수 : ' , eval_gini(y , oof_val_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "submission['target'] = oof_test_preds\n",
    "submission.to_csv('submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 성능 개선"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n       'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n       'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n       'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n       'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n       'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n       'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n       'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n       'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n       'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n       'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n       'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n       'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n       'ps_calc_20_bin'],\n      dtype='object')"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.concat([train , test] , ignore_index = True)\n",
    "all_data = all_data.drop('target' , axis =1 ) # 타깃값 제거\n",
    "\n",
    "all_features = all_data.columns # 전체 피처\n",
    "\n",
    "all_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 명목형 피처\n",
    "\n",
    "cat_features = [feature for feature in all_features if 'cat' in feature]\n",
    "\n",
    "# 원-핫 인코딩 적용\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "encoded_cat_matrix = onehot_encoder.fit_transform(all_data[cat_features])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 파생 피처 추가"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# 탐색적 데이터 분석과정에서는 필요 없는 피처를 추리는 것 외에 특별한 피처 엔지니어링 건을 찾아내지 못했다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# 첫번째 , 한 데이터가 가진 결측값 개수를 파생 피처로 만들어본다. -1 이 결측값이므로 결측값 개수를 구하려면 -1 개수를 구하면 된다.\n",
    "\n",
    "# 데이터 하나당 결측값 개수를 파생 피처로 추가\n",
    "\n",
    "all_data['num_missing'] = (all_data == -1).sum(axis = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "['ps_ind_01',\n 'ps_ind_03',\n 'ps_ind_06_bin',\n 'ps_ind_07_bin',\n 'ps_ind_08_bin',\n 'ps_ind_09_bin',\n 'ps_ind_10_bin',\n 'ps_ind_11_bin',\n 'ps_ind_12_bin',\n 'ps_ind_13_bin',\n 'ps_ind_14',\n 'ps_ind_15',\n 'ps_ind_16_bin',\n 'ps_ind_17_bin',\n 'ps_ind_18_bin',\n 'ps_reg_01',\n 'ps_reg_02',\n 'ps_reg_03',\n 'ps_car_11',\n 'ps_car_12',\n 'ps_car_13',\n 'ps_car_14',\n 'ps_car_15',\n 'num_missing']"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명목형 피처 , calc 분류의 피처를 제외한 피처\n",
    "\n",
    "remaining_features = [feature for feature in all_features if ('cat' not in feature and 'calc' not in feature)]\n",
    "\n",
    "# num_missing을 remaining_features 에 추가\n",
    "\n",
    "remaining_features.append('num_missing')\n",
    "\n",
    "remaining_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# 두번째, ind 분류의 피처들을 살펴본다. 모든 ind 피처 값을 연결해서 새로운 피처를 만들려고한다.\n",
    "\n",
    "# 예를들어 , ps_ind_01 , ps_ind_02_car , ps_ind_03의 값이 각각 2, 3, 5 라면 모든 값을 연결해 2_2_5_로 만든다.\n",
    "\n",
    "# ind 피처가 총 18개이므로 18개 값이 연결된다.\n",
    "\n",
    "ind_features = [feature for feature in all_features if 'ind' in feature]\n",
    "\n",
    "is_first_feature = True\n",
    "\n",
    "for ind_feature in ind_features:\n",
    "    if is_first_feature:\n",
    "        all_data['mix_ind'] = all_data[ind_feature].astype(str) + '-'\n",
    "        is_first_feature = False\n",
    "    else:\n",
    "        all_data['mix_ind'] += all_data[ind_feature].astype(str) + '-'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "0          2-2-5-1-0-0-1-0-0-0-0-0-0-0-11-0-1-0-\n1           1-1-7-0-0-0-0-1-0-0-0-0-0-0-3-0-0-1-\n2          5-4-9-1-0-0-0-1-0-0-0-0-0-0-12-1-0-0-\n3           0-1-2-0-0-1-0-0-0-0-0-0-0-0-8-1-0-0-\n4           0-2-0-1-0-1-0-0-0-0-0-0-0-0-9-1-0-0-\n                           ...                  \n1488023     0-1-6-0-0-0-1-0-0-0-0-0-0-0-2-0-0-1-\n1488024    5-3-5-1-0-0-0-1-0-0-0-0-0-0-11-1-0-0-\n1488025     0-1-5-0-0-1-0-0-0-0-0-0-0-0-5-0-0-1-\n1488026    6-1-5-1-0-0-0-0-1-0-0-0-0-0-13-1-0-0-\n1488027    7-1-4-1-0-0-0-0-1-0-0-0-0-0-12-1-0-0-\nName: mix_ind, Length: 1488028, dtype: object"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['mix_ind']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": " 1    1079327\n 2     309747\n 3      70172\n 4      28259\n-1        523\nName: ps_ind_02_cat, dtype: int64"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 세번째 , 명목형 피처의 고윳값별 개수를 새로운 피처로 추가한다. 고윳값별 개수는 value_counts()로 구한다.\n",
    "\n",
    "# ps_ind_02_cat 피처의 고윳값별 개수 코드\n",
    "\n",
    "all_data['ps_ind_02_cat'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "{1: 1079327, 2: 309747, 3: 70172, 4: 28259, -1: 523}"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명목형 피처의 고윳값별 개수를 파생 피처로 생성\n",
    "all_data['ps_ind_02_cat'].value_counts().to_dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "cat_count_features = []\n",
    "\n",
    "for feature in cat_features+['mix_ind']:\n",
    "    val_counts_dict = all_data[feature].value_counts().to_dict()\n",
    "    all_data[f'{feature}_count'] = all_data[feature].apply(lambda x: val_counts_dict[x])\n",
    "\n",
    "    cat_count_features.append(f'{feature}_count')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "['ps_ind_02_cat_count',\n 'ps_ind_04_cat_count',\n 'ps_ind_05_cat_count',\n 'ps_car_01_cat_count',\n 'ps_car_02_cat_count',\n 'ps_car_03_cat_count',\n 'ps_car_04_cat_count',\n 'ps_car_05_cat_count',\n 'ps_car_06_cat_count',\n 'ps_car_07_cat_count',\n 'ps_car_08_cat_count',\n 'ps_car_09_cat_count',\n 'ps_car_10_cat_count',\n 'ps_car_11_cat_count',\n 'mix_ind_count']"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_count_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 필요 없는 피처들\n",
    "drop_features = ['ps_ind_14' , 'ps_ind_10_bin' , 'ps_ind_11_bin' , 'ps_ind_12_bin' , 'ps_ind_13_bin' , 'ps_car_14']\n",
    "\n",
    "# remaining_features , cat_count_features 에서 drop_features를 제거한 데이터\n",
    "\n",
    "all_data_remaining = all_data[remaining_features + cat_count_features].drop(drop_features , axis = 1)\n",
    "\n",
    "# 데이터 합치기\n",
    "\n",
    "all_data_sprs = sparse.hstack([sparse.csr_matrix(all_data_remaining) , encoded_cat_matrix] ,format = 'csr')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 데이터 나누기"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "num_train = len(train) # 훈련 데이터 개수\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 나누기\n",
    "\n",
    "X = all_data_sprs[:num_train]\n",
    "X_test = all_data_sprs[num_train :]\n",
    "\n",
    "y = train['target'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 하이퍼파라미터 최적화"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8:2 비율로 훈련 데이터, 검증 데이터 분리(베이지안 최적화 수행용)\n",
    "\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X,y, test_size=0.2 , random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "\n",
    "bayes_dtrain = lgb.Dataset(X_train , y_train)\n",
    "bayes_dvalid = lgb.Dataset(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'num_leaves' : (30 , 40) ,\n",
    "                'lambda_l1' : (0.7 , 0.9),\n",
    "                'lambda_l2' : (0.9 , 1),\n",
    "                'feature_fraction' : (0.6 , 0.7),\n",
    "                'bagging_fraction' : (0.6 , 0.9),\n",
    "                'min_child_samples' : (6 , 10) ,\n",
    "                'min_child_weight' : (10 , 40)}\n",
    "\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "\n",
    "fixed_params = {'objective' : 'binary' ,\n",
    "                'learning_rate' : 0.005,\n",
    "                'bagging_freq' : 1,\n",
    "                'force_row_wise' : True,\n",
    "                'random_state' : 1991}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 베이지안 최적화용 평가지표 계산 함수 작성"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def eval_function(num_leaves , lambda_l1 , lambda_l2 , feature_fraction , bagging_fraction , min_child_samples , min_child_weight) :\n",
    "\n",
    "# 최적화하려는 평가지표(지니계수) 계산 함수\n",
    "\n",
    "# 베이지안 최적화를 수행할 하이퍼파라미터\n",
    "\n",
    "    params = {'num_leaves' : int(round(num_leaves)) ,\n",
    "              'lambda_l1' : lambda_l1,\n",
    "              'lambda_l2' : lambda_l2 ,\n",
    "              'feature_fraction' : feature_fraction ,\n",
    "              'bagging_fraction' : bagging_fraction,\n",
    "              'min_child_samples' : int(round(min_child_samples)) ,\n",
    "              'min_child_weight' : min_child_weight,\n",
    "              'feature_pre_filter' : False}\n",
    "\n",
    "    #하이퍼파라미터도 추가\n",
    "    params.update(fixed_params)\n",
    "\n",
    "    print('하이퍼파라미터 : ' , params)\n",
    "\n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params = params ,\n",
    "                          train_set = bayes_dtrain,\n",
    "                          num_boost_round= 2500,\n",
    "                          valid_sets= bayes_dvalid,\n",
    "                          feval = gini,\n",
    "                          early_stopping_rounds= 300,\n",
    "                          verbose_eval= False)\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = lgb_model.predict(X_valid)\n",
    "\n",
    "    # 지니계수 계산\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "\n",
    "    return gini_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 최적화 수행"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting colorama==0.4.4\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: colorama\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.6\n",
      "    Uninstalling colorama-0.4.6:\n",
      "      Successfully uninstalled colorama-0.4.6\n",
      "Successfully installed colorama-0.4.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylint 2.7.4 requires astroid<2.7,>=2.5.2, but you have astroid 2.5 which is incompatible.\n",
      "bayesian-optimization 1.4.2 requires colorama>=0.4.6, but you have colorama 0.4.4 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org colorama==0.4.4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization==1.4.0\n",
      "  Downloading bayesian_optimization-1.4.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bayesian-optimization==1.4.0) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bayesian-optimization==1.4.0) (1.20.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bayesian-optimization==1.4.0) (1.6.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization==1.4.0) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization==1.4.0) (1.0.1)\n",
      "Installing collected packages: bayesian-optimization\n",
      "  Attempting uninstall: bayesian-optimization\n",
      "    Found existing installation: bayesian-optimization 1.4.2\n",
      "    Uninstalling bayesian-optimization-1.4.2:\n",
      "      Successfully uninstalled bayesian-optimization-1.4.2\n",
      "Successfully installed bayesian-optimization-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org bayesian-optimization==1.4.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# 베이지안 최적화 객체 생성\n",
    "optimizer = BayesianOptimization(f = eval_function, # 평가지표 계산 함수\n",
    "                                pbounds = param_bounds, # 하이퍼파라미터 범위\n",
    "                                random_state = 0 )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 | min_ch... | min_ch... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "하이퍼파라미터 :  {'num_leaves': 34, 'lambda_l1': 0.8205526752143287, 'lambda_l2': 0.9544883182996897, 'feature_fraction': 0.6715189366372419, 'bagging_fraction': 0.7646440511781974, 'min_child_samples': 8, 'min_child_weight': 29.376823391999682, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2855811556220905\n",
      "\n",
      "| \u001B[0m1        \u001B[0m | \u001B[0m0.2856   \u001B[0m | \u001B[0m0.7646   \u001B[0m | \u001B[0m0.6715   \u001B[0m | \u001B[0m0.8206   \u001B[0m | \u001B[0m0.9545   \u001B[0m | \u001B[0m7.695    \u001B[0m | \u001B[0m29.38    \u001B[0m | \u001B[0m34.38    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 39, 'lambda_l1': 0.7766883037651555, 'lambda_l2': 0.9791725038082665, 'feature_fraction': 0.6963662760501029, 'bagging_fraction': 0.867531900234624, 'min_child_samples': 8, 'min_child_weight': 27.04133683281797, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2837380537005777\n",
      "\n",
      "| \u001B[0m2        \u001B[0m | \u001B[0m0.2837   \u001B[0m | \u001B[0m0.8675   \u001B[0m | \u001B[0m0.6964   \u001B[0m | \u001B[0m0.7767   \u001B[0m | \u001B[0m0.9792   \u001B[0m | \u001B[0m8.116    \u001B[0m | \u001B[0m27.04    \u001B[0m | \u001B[0m39.26    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 40, 'lambda_l1': 0.7040436794880651, 'lambda_l2': 0.9832619845547939, 'feature_fraction': 0.608712929970154, 'bagging_fraction': 0.6213108174593661, 'min_child_samples': 9, 'min_child_weight': 36.10036444740457, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n",
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2857848354322048\n",
      "\n",
      "| \u001B[95m3        \u001B[0m | \u001B[95m0.2858   \u001B[0m | \u001B[95m0.6213   \u001B[0m | \u001B[95m0.6087   \u001B[0m | \u001B[95m0.704    \u001B[0m | \u001B[95m0.9833   \u001B[0m | \u001B[95m9.113    \u001B[0m | \u001B[95m36.1     \u001B[0m | \u001B[95m39.79    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 30, 'lambda_l1': 0.8444997594874222, 'lambda_l2': 0.9234023852202012, 'feature_fraction': 0.6593983245038058, 'bagging_fraction': 0.8977977822397395, 'min_child_samples': 9, 'min_child_weight': 10.549362495448534, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2828993761731121\n",
      "\n",
      "| \u001B[0m4        \u001B[0m | \u001B[0m0.2829   \u001B[0m | \u001B[0m0.8978   \u001B[0m | \u001B[0m0.6594   \u001B[0m | \u001B[0m0.8445   \u001B[0m | \u001B[0m0.9234   \u001B[0m | \u001B[0m8.619    \u001B[0m | \u001B[0m10.55    \u001B[0m | \u001B[0m30.09    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 37, 'lambda_l1': 0.7738449330497988, 'lambda_l2': 0.9032695189818599, 'feature_fraction': 0.6606341064409726, 'bagging_fraction': 0.7666713964943057, 'min_child_samples': 9, 'min_child_weight': 29.306172421380474, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28513273331754563\n",
      "\n",
      "| \u001B[0m5        \u001B[0m | \u001B[0m0.2851   \u001B[0m | \u001B[0m0.7667   \u001B[0m | \u001B[0m0.6606   \u001B[0m | \u001B[0m0.7738   \u001B[0m | \u001B[0m0.9033   \u001B[0m | \u001B[0m8.769    \u001B[0m | \u001B[0m29.31    \u001B[0m | \u001B[0m36.6     \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 33, 'lambda_l1': 0.8247598808129755, 'lambda_l2': 0.9, 'feature_fraction': 0.6946301633473183, 'bagging_fraction': 0.6587307988780783, 'min_child_samples': 10, 'min_child_weight': 35.8584391833759, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2848579064930986\n",
      "\n",
      "| \u001B[0m6        \u001B[0m | \u001B[0m0.2849   \u001B[0m | \u001B[0m0.6587   \u001B[0m | \u001B[0m0.6946   \u001B[0m | \u001B[0m0.8248   \u001B[0m | \u001B[0m0.9      \u001B[0m | \u001B[0m9.818    \u001B[0m | \u001B[0m35.86    \u001B[0m | \u001B[0m32.8     \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 39, 'lambda_l1': 0.8720599498548245, 'lambda_l2': 0.9769965142032127, 'feature_fraction': 0.6770410324297845, 'bagging_fraction': 0.6283750235094676, 'min_child_samples': 6, 'min_child_weight': 39.88993616984878, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.2863207772581034\n",
      "\n",
      "| \u001B[95m7        \u001B[0m | \u001B[95m0.2863   \u001B[0m | \u001B[95m0.6284   \u001B[0m | \u001B[95m0.677    \u001B[0m | \u001B[95m0.8721   \u001B[0m | \u001B[95m0.977    \u001B[0m | \u001B[95m6.163    \u001B[0m | \u001B[95m39.89    \u001B[0m | \u001B[95m39.49    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 35, 'lambda_l1': 0.7881579420320822, 'lambda_l2': 0.9341538128137851, 'feature_fraction': 0.6337001812462025, 'bagging_fraction': 0.7425268652158652, 'min_child_samples': 6, 'min_child_weight': 39.820494958060664, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28668675475018845\n",
      "\n",
      "| \u001B[95m8        \u001B[0m | \u001B[95m0.2867   \u001B[0m | \u001B[95m0.7425   \u001B[0m | \u001B[95m0.6337   \u001B[0m | \u001B[95m0.7882   \u001B[0m | \u001B[95m0.9342   \u001B[0m | \u001B[95m6.036    \u001B[0m | \u001B[95m39.82    \u001B[0m | \u001B[95m35.13    \u001B[0m |\n",
      "하이퍼파라미터 :  {'num_leaves': 30, 'lambda_l1': 0.7942298723143388, 'lambda_l2': 0.9341906136292961, 'feature_fraction': 0.6014736366292884, 'bagging_fraction': 0.8901672711052453, 'min_child_samples': 6, 'min_child_weight': 39.73902941168508, 'feature_pre_filter': False, 'objective': 'binary', 'learning_rate': 0.005, 'bagging_freq': 1, 'force_row_wise': True, 'random_state': 1991}\n",
      "[LightGBM] [Info] Number of positive: 17383, number of negative: 458786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1555\n",
      "[LightGBM] [Info] Number of data points in the train set: 476169, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036506 -> initscore=-3.273091\n",
      "[LightGBM] [Info] Start training from score -3.273091\n",
      "지니계수 : 0.28486372926665016\n",
      "\n",
      "| \u001B[0m9        \u001B[0m | \u001B[0m0.2849   \u001B[0m | \u001B[0m0.8902   \u001B[0m | \u001B[0m0.6015   \u001B[0m | \u001B[0m0.7942   \u001B[0m | \u001B[0m0.9342   \u001B[0m | \u001B[0m6.199    \u001B[0m | \u001B[0m39.74    \u001B[0m | \u001B[0m30.33    \u001B[0m |\n",
      "=============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 베이지안 최적화 수행\n",
    "\n",
    "optimizer.maximize(init_points=  3 , n_iter = 6)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 결과확인"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "{'bagging_fraction': 0.7425268652158652,\n 'feature_fraction': 0.6337001812462025,\n 'lambda_l1': 0.7881579420320822,\n 'lambda_l2': 0.9341538128137851,\n 'min_child_samples': 6.03624517464404,\n 'min_child_weight': 39.820494958060664,\n 'num_leaves': 35.128706389767935}"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가함수 점수가 최대일 대 하이퍼파라미터\n",
    "max_params = optimizer.max['params']\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# 정수형 하이퍼파라미터 변환\n",
    "\n",
    "max_params['num_leaves'] = int(round(max_params['num_leaves']))\n",
    "max_params['min_child_samples'] = int(round(max_params['min_child_samples']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# 값이 고정된 하이퍼파라미터 추가\n",
    "\n",
    "max_params.update(fixed_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "{'bagging_fraction': 0.7425268652158652,\n 'feature_fraction': 0.6337001812462025,\n 'lambda_l1': 0.7881579420320822,\n 'lambda_l2': 0.9341538128137851,\n 'min_child_samples': 6,\n 'min_child_weight': 39.820494958060664,\n 'num_leaves': 35,\n 'objective': 'binary',\n 'learning_rate': 0.005,\n 'bagging_freq': 1,\n 'force_row_wise': True,\n 'random_state': 1991}"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최종 하이퍼파라미터 출력\n",
    "max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 훈련 및 성능 검증"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## 폴드 1 / 폴드 5 ########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153987\tvalid_0's gini: 0.292724\n",
      "[200]\tvalid_0's binary_logloss: 0.152688\tvalid_0's gini: 0.301543\n",
      "[300]\tvalid_0's binary_logloss: 0.151878\tvalid_0's gini: 0.309923\n",
      "[400]\tvalid_0's binary_logloss: 0.151302\tvalid_0's gini: 0.317712\n",
      "[500]\tvalid_0's binary_logloss: 0.150854\tvalid_0's gini: 0.325174\n",
      "[600]\tvalid_0's binary_logloss: 0.150492\tvalid_0's gini: 0.331907\n",
      "[700]\tvalid_0's binary_logloss: 0.150169\tvalid_0's gini: 0.338587\n",
      "[800]\tvalid_0's binary_logloss: 0.149889\tvalid_0's gini: 0.34475\n",
      "[900]\tvalid_0's binary_logloss: 0.149636\tvalid_0's gini: 0.350422\n",
      "[1000]\tvalid_0's binary_logloss: 0.14941\tvalid_0's gini: 0.35556\n",
      "[1100]\tvalid_0's binary_logloss: 0.149189\tvalid_0's gini: 0.360673\n",
      "[1200]\tvalid_0's binary_logloss: 0.148983\tvalid_0's gini: 0.365316\n",
      "[1300]\tvalid_0's binary_logloss: 0.148791\tvalid_0's gini: 0.369922\n",
      "[1400]\tvalid_0's binary_logloss: 0.148609\tvalid_0's gini: 0.374208\n",
      "[1500]\tvalid_0's binary_logloss: 0.148422\tvalid_0's gini: 0.378589\n",
      "[1600]\tvalid_0's binary_logloss: 0.148245\tvalid_0's gini: 0.382737\n",
      "[1700]\tvalid_0's binary_logloss: 0.148071\tvalid_0's gini: 0.386937\n",
      "[1800]\tvalid_0's binary_logloss: 0.147903\tvalid_0's gini: 0.39092\n",
      "[1900]\tvalid_0's binary_logloss: 0.147734\tvalid_0's gini: 0.394907\n",
      "[2000]\tvalid_0's binary_logloss: 0.14757\tvalid_0's gini: 0.398723\n",
      "[2100]\tvalid_0's binary_logloss: 0.147413\tvalid_0's gini: 0.402418\n",
      "[2200]\tvalid_0's binary_logloss: 0.147249\tvalid_0's gini: 0.406203\n",
      "[2300]\tvalid_0's binary_logloss: 0.147098\tvalid_0's gini: 0.40973\n",
      "[2400]\tvalid_0's binary_logloss: 0.146948\tvalid_0's gini: 0.413188\n",
      "[2500]\tvalid_0's binary_logloss: 0.146805\tvalid_0's gini: 0.416567\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's binary_logloss: 0.146805\tvalid_0's gini: 0.416567\n",
      "폴드 1  지니계수 : 0.4165668891543515\n",
      "\n",
      "######################################## 폴드 2 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.15408\tvalid_0's gini: 0.281299\n",
      "[200]\tvalid_0's binary_logloss: 0.152841\tvalid_0's gini: 0.290897\n",
      "[300]\tvalid_0's binary_logloss: 0.152086\tvalid_0's gini: 0.298854\n",
      "[400]\tvalid_0's binary_logloss: 0.151555\tvalid_0's gini: 0.306219\n",
      "[500]\tvalid_0's binary_logloss: 0.151147\tvalid_0's gini: 0.31311\n",
      "[600]\tvalid_0's binary_logloss: 0.150806\tvalid_0's gini: 0.319581\n",
      "[700]\tvalid_0's binary_logloss: 0.150505\tvalid_0's gini: 0.325852\n",
      "[800]\tvalid_0's binary_logloss: 0.150231\tvalid_0's gini: 0.331772\n",
      "[900]\tvalid_0's binary_logloss: 0.14999\tvalid_0's gini: 0.337231\n",
      "[1000]\tvalid_0's binary_logloss: 0.149761\tvalid_0's gini: 0.342421\n",
      "[1100]\tvalid_0's binary_logloss: 0.149543\tvalid_0's gini: 0.347403\n",
      "[1200]\tvalid_0's binary_logloss: 0.149335\tvalid_0's gini: 0.352265\n",
      "[1300]\tvalid_0's binary_logloss: 0.149137\tvalid_0's gini: 0.356916\n",
      "[1400]\tvalid_0's binary_logloss: 0.148949\tvalid_0's gini: 0.361308\n",
      "[1500]\tvalid_0's binary_logloss: 0.148768\tvalid_0's gini: 0.36546\n",
      "[1600]\tvalid_0's binary_logloss: 0.14859\tvalid_0's gini: 0.369617\n",
      "[1700]\tvalid_0's binary_logloss: 0.148417\tvalid_0's gini: 0.373906\n",
      "[1800]\tvalid_0's binary_logloss: 0.148245\tvalid_0's gini: 0.377886\n",
      "[1900]\tvalid_0's binary_logloss: 0.148079\tvalid_0's gini: 0.381759\n",
      "[2000]\tvalid_0's binary_logloss: 0.147916\tvalid_0's gini: 0.385494\n",
      "[2100]\tvalid_0's binary_logloss: 0.147751\tvalid_0's gini: 0.389415\n",
      "[2200]\tvalid_0's binary_logloss: 0.147591\tvalid_0's gini: 0.392991\n",
      "[2300]\tvalid_0's binary_logloss: 0.147437\tvalid_0's gini: 0.396485\n",
      "[2400]\tvalid_0's binary_logloss: 0.147286\tvalid_0's gini: 0.400034\n",
      "[2500]\tvalid_0's binary_logloss: 0.147128\tvalid_0's gini: 0.403878\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's binary_logloss: 0.147128\tvalid_0's gini: 0.403878\n",
      "폴드 2  지니계수 : 0.40387803708259545\n",
      "\n",
      "######################################## 폴드 3 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.153969\tvalid_0's gini: 0.287114\n",
      "[200]\tvalid_0's binary_logloss: 0.15266\tvalid_0's gini: 0.29621\n",
      "[300]\tvalid_0's binary_logloss: 0.151845\tvalid_0's gini: 0.304396\n",
      "[400]\tvalid_0's binary_logloss: 0.151276\tvalid_0's gini: 0.311883\n",
      "[500]\tvalid_0's binary_logloss: 0.150844\tvalid_0's gini: 0.318519\n",
      "[600]\tvalid_0's binary_logloss: 0.150498\tvalid_0's gini: 0.324701\n",
      "[700]\tvalid_0's binary_logloss: 0.150195\tvalid_0's gini: 0.330726\n",
      "[800]\tvalid_0's binary_logloss: 0.149929\tvalid_0's gini: 0.336336\n",
      "[900]\tvalid_0's binary_logloss: 0.149693\tvalid_0's gini: 0.341468\n",
      "[1000]\tvalid_0's binary_logloss: 0.149475\tvalid_0's gini: 0.346234\n",
      "[1100]\tvalid_0's binary_logloss: 0.149269\tvalid_0's gini: 0.350908\n",
      "[1200]\tvalid_0's binary_logloss: 0.149067\tvalid_0's gini: 0.355514\n",
      "[1300]\tvalid_0's binary_logloss: 0.14888\tvalid_0's gini: 0.359948\n",
      "[1400]\tvalid_0's binary_logloss: 0.148704\tvalid_0's gini: 0.364075\n",
      "[1500]\tvalid_0's binary_logloss: 0.148533\tvalid_0's gini: 0.368009\n",
      "[1600]\tvalid_0's binary_logloss: 0.14836\tvalid_0's gini: 0.371999\n",
      "[1700]\tvalid_0's binary_logloss: 0.148197\tvalid_0's gini: 0.375947\n",
      "[1800]\tvalid_0's binary_logloss: 0.148033\tvalid_0's gini: 0.379777\n",
      "[1900]\tvalid_0's binary_logloss: 0.147872\tvalid_0's gini: 0.383514\n",
      "[2000]\tvalid_0's binary_logloss: 0.147717\tvalid_0's gini: 0.387124\n",
      "[2100]\tvalid_0's binary_logloss: 0.147567\tvalid_0's gini: 0.390669\n",
      "[2200]\tvalid_0's binary_logloss: 0.147409\tvalid_0's gini: 0.394312\n",
      "[2300]\tvalid_0's binary_logloss: 0.147252\tvalid_0's gini: 0.397918\n",
      "[2400]\tvalid_0's binary_logloss: 0.147107\tvalid_0's gini: 0.401252\n",
      "[2500]\tvalid_0's binary_logloss: 0.14696\tvalid_0's gini: 0.404791\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's binary_logloss: 0.14696\tvalid_0's gini: 0.404791\n",
      "폴드 3  지니계수 : 0.4047906440895664\n",
      "\n",
      "######################################## 폴드 4 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154101\tvalid_0's gini: 0.277771\n",
      "[200]\tvalid_0's binary_logloss: 0.152844\tvalid_0's gini: 0.287995\n",
      "[300]\tvalid_0's binary_logloss: 0.152087\tvalid_0's gini: 0.295637\n",
      "[400]\tvalid_0's binary_logloss: 0.151549\tvalid_0's gini: 0.302844\n",
      "[500]\tvalid_0's binary_logloss: 0.151147\tvalid_0's gini: 0.309379\n",
      "[600]\tvalid_0's binary_logloss: 0.150813\tvalid_0's gini: 0.315633\n",
      "[700]\tvalid_0's binary_logloss: 0.15052\tvalid_0's gini: 0.32166\n",
      "[800]\tvalid_0's binary_logloss: 0.150262\tvalid_0's gini: 0.327309\n",
      "[900]\tvalid_0's binary_logloss: 0.150017\tvalid_0's gini: 0.333044\n",
      "[1000]\tvalid_0's binary_logloss: 0.149798\tvalid_0's gini: 0.338138\n",
      "[1100]\tvalid_0's binary_logloss: 0.149597\tvalid_0's gini: 0.342844\n",
      "[1200]\tvalid_0's binary_logloss: 0.149396\tvalid_0's gini: 0.347605\n",
      "[1300]\tvalid_0's binary_logloss: 0.149212\tvalid_0's gini: 0.352048\n",
      "[1400]\tvalid_0's binary_logloss: 0.149039\tvalid_0's gini: 0.35632\n",
      "[1500]\tvalid_0's binary_logloss: 0.148862\tvalid_0's gini: 0.360523\n",
      "[1600]\tvalid_0's binary_logloss: 0.148692\tvalid_0's gini: 0.364672\n",
      "[1700]\tvalid_0's binary_logloss: 0.148525\tvalid_0's gini: 0.368846\n",
      "[1800]\tvalid_0's binary_logloss: 0.148356\tvalid_0's gini: 0.372869\n",
      "[1900]\tvalid_0's binary_logloss: 0.148198\tvalid_0's gini: 0.376635\n",
      "[2000]\tvalid_0's binary_logloss: 0.148041\tvalid_0's gini: 0.380378\n",
      "[2100]\tvalid_0's binary_logloss: 0.147888\tvalid_0's gini: 0.384133\n",
      "[2200]\tvalid_0's binary_logloss: 0.147737\tvalid_0's gini: 0.387622\n",
      "[2300]\tvalid_0's binary_logloss: 0.147579\tvalid_0's gini: 0.391321\n",
      "[2400]\tvalid_0's binary_logloss: 0.147429\tvalid_0's gini: 0.394841\n",
      "[2500]\tvalid_0's binary_logloss: 0.14728\tvalid_0's gini: 0.398467\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2500]\tvalid_0's binary_logloss: 0.14728\tvalid_0's gini: 0.398467\n",
      "폴드 4  지니계수 : 0.39846717411011123\n",
      "\n",
      "######################################## 폴드 5 / 폴드 5 ########################################\n",
      "[LightGBM] [Info] Number of positive: 17355, number of negative: 458815\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 476170, number of used features: 217\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036447 -> initscore=-3.274766\n",
      "[LightGBM] [Info] Start training from score -3.274766\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[100]\tvalid_0's binary_logloss: 0.154427\tvalid_0's gini: 0.263095\n",
      "[200]\tvalid_0's binary_logloss: 0.153428\tvalid_0's gini: 0.269457\n",
      "[300]\tvalid_0's binary_logloss: 0.15288\tvalid_0's gini: 0.273613\n",
      "[400]\tvalid_0's binary_logloss: 0.152551\tvalid_0's gini: 0.277158\n",
      "[500]\tvalid_0's binary_logloss: 0.15233\tvalid_0's gini: 0.280534\n",
      "[600]\tvalid_0's binary_logloss: 0.152173\tvalid_0's gini: 0.283537\n",
      "[700]\tvalid_0's binary_logloss: 0.152051\tvalid_0's gini: 0.286257\n",
      "[800]\tvalid_0's binary_logloss: 0.151957\tvalid_0's gini: 0.288418\n",
      "[900]\tvalid_0's binary_logloss: 0.151893\tvalid_0's gini: 0.290057\n",
      "[1000]\tvalid_0's binary_logloss: 0.151849\tvalid_0's gini: 0.291266\n",
      "[1100]\tvalid_0's binary_logloss: 0.151806\tvalid_0's gini: 0.292433\n",
      "[1200]\tvalid_0's binary_logloss: 0.15177\tvalid_0's gini: 0.293465\n",
      "[1300]\tvalid_0's binary_logloss: 0.151754\tvalid_0's gini: 0.293927\n",
      "[1400]\tvalid_0's binary_logloss: 0.151739\tvalid_0's gini: 0.294366\n",
      "[1500]\tvalid_0's binary_logloss: 0.151722\tvalid_0's gini: 0.294835\n",
      "[1600]\tvalid_0's binary_logloss: 0.151711\tvalid_0's gini: 0.295208\n",
      "[1700]\tvalid_0's binary_logloss: 0.1517\tvalid_0's gini: 0.295476\n",
      "[1800]\tvalid_0's binary_logloss: 0.151693\tvalid_0's gini: 0.295694\n",
      "[1900]\tvalid_0's binary_logloss: 0.151688\tvalid_0's gini: 0.295862\n",
      "[2000]\tvalid_0's binary_logloss: 0.151687\tvalid_0's gini: 0.295947\n",
      "[2100]\tvalid_0's binary_logloss: 0.151686\tvalid_0's gini: 0.296049\n",
      "[2200]\tvalid_0's binary_logloss: 0.151684\tvalid_0's gini: 0.296065\n",
      "[2300]\tvalid_0's binary_logloss: 0.15168\tvalid_0's gini: 0.296174\n",
      "[2400]\tvalid_0's binary_logloss: 0.151681\tvalid_0's gini: 0.296211\n",
      "[2500]\tvalid_0's binary_logloss: 0.151682\tvalid_0's gini: 0.296251\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2256]\tvalid_0's binary_logloss: 0.151678\tvalid_0's gini: 0.296173\n",
      "폴드 5  지니계수 : 0.29617346153678703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 층화 K 폴드 교차 검증기 생성\n",
    "folds = StratifiedKFold(n_splits=5 , shuffle = True , random_state= 1991)\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 검증 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "\n",
    "oof_val_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# OOF 방식으로 훈련된 모델로 테스트 데이터 타깃값을 예측한 확률을 담을 1차원 배열\n",
    "oof_test_preds = np.zeros(X_test.shape[0])\n",
    "\n",
    "# OOF 방식으로 모델 훈련 ,검증 , 예측\n",
    "\n",
    "for idx, (train , valid_idx) in enumerate(folds.split(X,y)):\n",
    "    # 각 폴드를 구분하는 문구 출력\n",
    "    print('#'*40 , f'폴드 {idx+1} / 폴드 {folds.n_splits}' , '#'*40)\n",
    "\n",
    "    X_train , y_train = X[train_idx] , y[train_idx] # 훈련용 데이터\n",
    "    X_valid , y_valid = X[valid_idx] , y[valid_idx] # 검증용 데이터\n",
    "\n",
    "    # LightGBM 전용 데이터셋 생성\n",
    "    dtrain = lgb.Dataset(X_train , y_train) # LightGBM 전용 훈련 데이터셋\n",
    "    dvalid = lgb.Dataset(X_valid , y_valid) # LightGBM 전용 검증 데이터셋\n",
    "\n",
    "    # LightGBM 모델 훈련\n",
    "    lgb_model = lgb.train(params = max_params , # 최적 하이퍼파라미터\n",
    "                          train_set = dtrain, # 훈련 데이터 셋\n",
    "                          num_boost_round= 2500, # 부스팅 반복 횟수\n",
    "                          valid_sets= dvalid , # 성능 평가용 검증 데이터셋\n",
    "                          feval = gini, # 검증용 평가지표\n",
    "                          early_stopping_rounds= 300, # 조기종료 조건\n",
    "                          verbose_eval = 100) # 100 번째 마다 점수 출력\n",
    "\n",
    "    # 테스트 데이터를 활용해 OOF 예측\n",
    "    oof_test_preds += lgb_model.predict(X_test) / folds.n_splits\n",
    "\n",
    "    # 모델 성능 평가를 위한 검증 데이터 타깃값 예측\n",
    "\n",
    "    oof_val_preds[valid_idx] += lgb_model.predict(X_valid)\n",
    "\n",
    "    # 검증 데이터 예측 확률에 대한 정규화 지니계수\n",
    "    gini_score = eval_gini(y_valid, oof_val_preds[valid_idx])\n",
    "    print(f'폴드 {idx+1}  지니계수 : {gini_score}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-78-6dde8cbfdaa4>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'OOF 검증 데이터 지니계수 :'\u001B[0m \u001B[1;33m,\u001B[0m \u001B[0meval_gini\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moof_val_preds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-10-b5db2767ba2c>\u001B[0m in \u001B[0;36meval_gini\u001B[1;34m(y_true, y_pred)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0meval_gini\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m \u001B[1;33m,\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;31m# 실제값과 예측값의 크기가 서로 같은지 확인(값이 다르면 오류 발생)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     \u001B[1;32massert\u001B[0m \u001B[0my_true\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0my_pred\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[0mn_samples\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0my_true\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;31m# 데이터 개수\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print('OOF 검증 데이터 지니계수 :' , eval_gini(y, oof_val_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost 피처 엔지니어링"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# LightGBM 용 gini() 함수\n",
    "def gini(preds , dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini' , eval_gini(labels , preds) , True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# XGBoost용 gini() 함수\n",
    "\n",
    "def gini(preds , dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'gini' , eval_gini(labels,preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 하이퍼파라미터 최적화"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 데이터 셋 준비"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8:2 비율로 훈련 데이터 , 검증 데이터 분리( 베이지안 최적화 수행용)\n",
    "\n",
    "X_train , X_valid , y_train , y_valid = train_test_split(X,y, test_size=0.2 , random_state=0)\n",
    "\n",
    "# 베이지안 최적화용 데이터셋\n",
    "\n",
    "bayes_dtrain = xgb.Dmatrix(X_train , y_train)\n",
    "bayes_dvalid = xgb.Dmatrix(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 하이퍼파라미터 범위 설정"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 베이지안 최적화를 위한 하이퍼파라미터 범위\n",
    "param_bounds = {'max_depth' : (4 , 8) ,\n",
    "                'subsample' : (0.6 , 0.9),\n",
    "                'colsample_bytree' : (0.7 , 1.0),\n",
    "                'min_child_weight' : (5 , 7),\n",
    "                'gamma' : (8 , 11),\n",
    "                'reg_alpha' : (7 , 9) ,\n",
    "                'reg_lambda' : (1.1 , 1.5),\n",
    "                'scale_pos_weight' : (1.4 , 1.6)}\n",
    "\n",
    "\n",
    "# 값이 고정된 하이퍼파라미터\n",
    "\n",
    "fixed_params = {'objective' : 'binary : logistic' ,\n",
    "                'learning_rate' : 0.02,\n",
    "                'random_state' : 1991}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def eval_function(max_depth , subsample , colsample_bytree , min_child_weight , reg_alpha , gamma , reg_lambda , scale_pos_weight) :\n",
    "\n",
    "    # 최적화하려는 평가지표(지니계수) 계산 함수\n",
    "\n",
    "    # 베이지안 최적화를 수행할 하이퍼파라미터\n",
    "\n",
    "    params = {'max_depth' : int(round(max_depth)) ,\n",
    "              'subsample' : subsample,\n",
    "              'colsample_bytree' : colsample_bytree ,\n",
    "              'min_child_weight' : min_child_weight,\n",
    "              'gamma' : gamma,\n",
    "              'reg_alpha' : reg_alpha,\n",
    "              'reg_lambda' : reg_lambda,\n",
    "              'scale_pos_weight' : scale_pos_weight}\n",
    "\n",
    "    # 값이 고정된 하이퍼파라미터도 추가\n",
    "    params.update(fixed_params)\n",
    "\n",
    "    print('하이퍼파라미터 : ' , params)\n",
    "\n",
    "    # XGBoost 모델 훈련\n",
    "    xgb_model = xgb.train(params = params ,\n",
    "                          dtrain = bayes_dtrain,\n",
    "                          num_boost_round= 2000,\n",
    "                          evals = [(bayes_dvalid , ' bayes_dvalid')],\n",
    "\n",
    "                          ,feval = gini,\n",
    "                          early_stopping_rounds= 300,\n",
    "                          verbose_eval= False)\n",
    "    # 검증 데이터로 예측 수행\n",
    "    preds = lgb_model.predict(X_valid)\n",
    "\n",
    "    # 지니계수 계산\n",
    "    gini_score = eval_gini(y_valid, preds)\n",
    "    print(f'지니계수 : {gini_score}\\n')\n",
    "\n",
    "    return gini_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}